{
 "paragraphs": [
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "IS_INTELLIJ_SERVICE": true,
      "ZTOOLS_DEBUG_CELL_ID": "97df5d43-65b8-4a9f-bfd4-10483cc1dce8"
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "ABORT",
   "text": "%spark.spark\n//ZToolsId = 97df5d43-65b8-4a9f-bfd4-10483cc1dce8\n// It is generated code for integration with Big Data Tools plugin\n// Please DO NOT edit it.\ntry {\n  import org.apache.commons.lang.exception.ExceptionUtils\n  import org.apache.spark.sql.SparkSession\n\n  import java.io.{PrintWriter, StringWriter}\n  import java.util\n  import scala.collection.mutable.ListBuffer\n  import scala.collection.{immutable, mutable}\n  import scala.reflect.api.JavaUniverse\n  import scala.tools.nsc.interpreter.IMain\n  import org.json4s.jackson.Serialization\n  import org.json4s.{Formats, NoTypeHints}\n\n  import java.util.function.{Function => JFunction}\n  import java.util.regex.Pattern\n  import scala.language.implicitConversions\n  import scala.util.Try\n  import org.apache.spark.sql.Dataset\n  import org.apache.spark.rdd.RDD\n  import org.apache.spark.SparkContext\n\n  trait Loopback {\n    def pass(obj: Any, id: String): Any\n  }\n\n  object ResNames {\n    val REF = \"ref\"\n    val VALUE = \"value\"\n    val IS_PRIMITIVE = \"isPrimitive\"\n    val TYPE = \"type\"\n    val TIME = \"time\"\n    val LENGTH = \"length\"\n    val LAZY = \"lazy\"\n  }\n\n  object TrieMap {\n    class Node[T](var value: Option[T]) {\n      var children: mutable.Map[String, TrieMap.Node[T]] = _\n\n      def put(key: String, node: TrieMap.Node[T]): Option[Node[T]] = {\n        if (children == null)\n          children = mutable.Map[String, TrieMap.Node[T]]()\n        children.put(key, node)\n      }\n\n      def del(key: String): Option[Node[T]] = children.remove(key)\n\n      def forEach(func: Function[T, _]): Unit = {\n        func.apply(value.get)\n        if (children != null) children.foreach(t => t._2.forEach(func))\n      }\n    }\n\n    def split(key: String): Array[String] = {\n      var n = 0\n      var j = 0\n      for (i <- 0 until key.length) {\n        if (key.charAt(i) == '.') n += 1\n      }\n      val k = new Array[String](n + 1)\n      val sb = new mutable.StringBuilder(k.length)\n      for (i <- 0 until key.length) {\n        val ch = key.charAt(i)\n        if (ch == '.') {\n          k({\n            j += 1;\n            j - 1\n          }) = sb.toString\n          sb.setLength(0)\n        }\n        else sb.append(ch)\n      }\n      k(j) = sb.toString\n      k\n    }\n  }\n\n  class TrieMap[T] {\n    val root = new TrieMap.Node[T](null)\n\n    def subtree(key: Array[String], length: Int): TrieMap.Node[T] = {\n      var current = root\n      var i = 0\n      while ( {\n        i < length && current != null\n      }) {\n        if (current.children == null) return null\n        current = current.children.get(key(i)).orNull\n        i += 1\n      }\n      current\n    }\n\n    def put(key: Array[String], value: T): Option[TrieMap.Node[T]] = {\n      val node = subtree(key, key.length - 1)\n      node.put(key(key.length - 1), new TrieMap.Node[T](Option.apply(value)))\n    }\n\n    def put(key: String, value: T): Option[TrieMap.Node[T]] = {\n      val k = TrieMap.split(key)\n      put(k, value)\n    }\n\n    def contains(key: String): Boolean = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      node != null\n    }\n\n    def get(key: String): Option[T] = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      if (node == null) return Option.empty\n      node.value\n    }\n\n    def subtree(key: String): TrieMap.Node[T] = {\n      val k = TrieMap.split(key)\n      subtree(k, k.length)\n    }\n  }\n\n  trait TypeHandler {\n    def accept(obj: Any): Boolean\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any]\n\n    def getErrors: List[String] = List[String]()\n  }\n\n  abstract class AbstractCollectionHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    trait Iterator {\n      def hasNext: Boolean\n\n      def next: Any\n    }\n\n    def iterator(obj: Any): Iterator\n\n    def length(obj: Any): Int\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      if (depth <= 0) {\n        withJsonObject { result =>\n          var s = scalaInfo.value.toString\n          if (s.length>1000)\n            s = s.take(1000) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n      } else {\n        mutable.Map[String, Any](\n          ResNames.LENGTH -> length(scalaInfo.value),\n          ResNames.VALUE -> withJsonArray { json =>\n            val startTime = System.currentTimeMillis()\n            val it = iterator(scalaInfo.value)\n            var index = 0\n            while (it.hasNext && index < limit && !checkTimeoutError(scalaInfo.path, startTime, timeout)) {\n                val id = scalaInfo.path\n                json += loopback.pass(it.next, s\"$id[$index]\")\n                index += 1\n            }\n            })\n      }\n    }\n  }\n\n  abstract class AbstractTypeHandler extends TypeHandler {\n    val timeoutErrors: mutable.MutableList[String] = mutable.MutableList()\n\n    override def getErrors: List[String] = timeoutErrors.toList\n\n    protected def withJsonArray(body: mutable.MutableList[Any] => Unit): mutable.MutableList[Any] = {\n      val arr = mutable.MutableList[Any]()\n      body(arr)\n      arr\n    }\n\n    protected def withJsonObject(body: mutable.Map[String, Any] => Unit): mutable.Map[String, Any] = {\n      val obj = mutable.Map[String, Any]()\n      body(obj)\n      obj\n    }\n\n    protected def wrap(obj: Any, tpe: String): mutable.Map[String, Any] = mutable.Map[String, Any](\n      ResNames.VALUE -> Option(obj).orNull,\n      ResNames.TYPE -> tpe\n    )\n\n    protected def checkTimeoutError(name: String, startTime: Long, timeout: Int): Boolean = {\n      val isTimeout = System.currentTimeMillis() - startTime > timeout\n      if (isTimeout)\n        timeoutErrors += f\"Variable $name collect timeout exceed ${timeout}ms.\"\n      isTimeout\n    }\n\n  }\n\n  class ArrayHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Array[_]]\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Array[_]].length\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Array[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next\n    }\n  }\n\n  class JavaCollectionHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[util.Collection[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator() {\n      private val it = obj.asInstanceOf[util.Collection[_]].iterator()\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[util.Collection[_]].size()\n  }\n  class MapHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject {\n        json =>\n          val obj = scalaInfo.value\n          val id = scalaInfo.path\n          val map = obj.asInstanceOf[Map[_, _]]\n          val keys = mutable.MutableList[Any]()\n          val values = mutable.MutableList[Any]()\n          json += (\"jvm-type\" -> obj.getClass.getCanonicalName)\n          json += (\"length\" -> map.size)\n          var index = 0\n\n          json += (\"key\" -> keys)\n          json += (\"value\" -> values)\n\n          val startTime = System.currentTimeMillis()\n          map.view.take(math.min(limit, map.size)).foreach {\n            case (key, value) =>\n              if (checkTimeoutError(scalaInfo.path, startTime, timeout))\n                return json\n              keys += loopback.pass(key, s\"$id.key[$index]\")\n              values += loopback.pass(value, s\"$id.value[$index]\")\n              index += 1\n          }\n      }\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Map[_, _]]\n  }\n\n  class NullHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj == null\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any]()\n  }\n\n  class ObjectHandler(val stringSizeLimit: Int,\n                      val manager: HandlerManager,\n                      val referenceManager: ReferenceManager,\n                      val timeout: Int) extends AbstractTypeHandler {\n    private val INACCESSIBLE = ScalaVariableInfo(isAccessible = false, isLazy = false, null, null, null, null)\n    val ru: JavaUniverse = scala.reflect.runtime.universe\n    val mirror: ru.Mirror = ru.runtimeMirror(getClass.getClassLoader)\n    import scala.reflect.runtime.universe.NoSymbol\n    case class ReflectionProblem(e: Throwable, symbol: String, var count: Int)\n\n    val problems: mutable.Map[String, ReflectionProblem] = mutable.Map[String, ReflectionProblem]()\n\n    override def accept(obj: Any): Boolean = true\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject { result =>\n        val obj = scalaInfo.value\n\n        if (obj == null) {\n          return result\n        }\n        if (depth <= 0) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val startTime = System.currentTimeMillis()\n        val fields = listAccessibleProperties(scalaInfo, startTime)\n        if (fields.isEmpty) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val resolvedFields = mutable.Map[String, Any]()\n        result += (ResNames.VALUE -> resolvedFields)\n\n\n        fields.foreach { field =>\n          if (checkTimeoutError(field.name, startTime, timeout)) {\n            return result\n          }\n\n          if (field.ref != null && field.ref != field.path) {\n            resolvedFields += (field.name -> (mutable.Map[String, Any]() += (ResNames.REF -> field.ref)))\n          } else {\n            resolvedFields += (field.name -> manager.handleVariable(field, loopback, depth - 1))\n          }\n        }\n\n        result\n      }\n\n\n    override def getErrors: List[String] = problems.map(x =>\n      f\"Reflection error for ${x._2.symbol} counted ${x._2.count}.\\n\" +\n        f\"Error message: ${ExceptionUtils.getMessage(x._2.e)}\\n \" +\n        f\"Stacktrace:${ExceptionUtils.getStackTrace(x._2.e)}\").toList ++ super.getErrors\n\n    private def listAccessibleProperties(info: ScalaVariableInfo, startTime: Long): List[ScalaVariableInfo] = {\n      val instanceMirror = mirror.reflect(info.value)\n      val instanceSymbol = instanceMirror.symbol\n      val members = instanceSymbol.toType.members\n\n      val parsedMembers = mutable.MutableList[ScalaVariableInfo]()\n      members.foreach { symbol =>\n        if (checkTimeoutError(info.path, startTime, timeout))\n          return parsedMembers.toList\n        val variableInfo = get(instanceMirror, symbol, info.path)\n        if (variableInfo.isAccessible)\n          parsedMembers += variableInfo\n      }\n\n      parsedMembers.toList\n    }\n\n    private def get(instanceMirror: ru.InstanceMirror, symbol: ru.Symbol, path: String): ScalaVariableInfo = {\n      if (!problems.contains(path))\n        try {\n          // is public property\n          if (!symbol.isMethod && symbol.isTerm && (symbol.asTerm.isVar || symbol.asTerm.isVal)\n          && symbol.asTerm.getter != NoSymbol\n          && symbol.asTerm.getter.isPublic) {\n            val term = symbol.asTerm\n            val f = instanceMirror.reflectField(term)\n            val fieldPath = s\"$path.${term.name.toString.trim}\"\n            val value = f.get\n            val tpe = term.typeSignature.toString\n            return ScalaVariableInfo(isAccessible = tpe != \"<notype>\", isLazy = term.isLazy, value, tpe,\n              fieldPath, referenceManager.getRef(value, fieldPath))\n          }\n        } catch {\n          case e: Throwable => problems(path) = ReflectionProblem(e, symbol.toString, 1)\n        }\n      else\n        problems(path).count += 1\n\n      INACCESSIBLE\n    }\n  }\n\n  class PrimitiveHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean =\n      obj match {\n        case _: Byte => true\n        case _: Short => true\n        case _: Boolean => true\n        case _: Char => true\n        case _: Int => true\n        case _: Long => true\n        case _: Float => true\n        case _: Double => true\n        case _ => false\n      }\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any](\n        ResNames.VALUE -> scalaInfo.value,\n        ResNames.IS_PRIMITIVE -> 1\n      )\n  }\n\n  class SeqHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Seq[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Seq[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Seq[_]].size\n  }\n\n  class SetHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Set[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Set[_]].size\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Set[_]]\n  }\n\n  class SpecialsHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.getClass.getCanonicalName != null && obj.getClass.getCanonicalName.startsWith(\"scala.\")\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        var s = scalaInfo.value.toString\n        if (s.length>limit)\n          s = s.take(limit) + \"...\"\n        json.put(ResNames.VALUE, s)\n    }\n  }\n\n  class StringHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[String]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      var s = scalaInfo.value.asInstanceOf[String]\n      if (s.length>limit)\n        s = s.take(limit) + \"...\"\n      mutable.Map(\n        ResNames.VALUE -> s\n      )\n    }\n  }\n\n  class ThrowableHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Throwable]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val throwable = obj.asInstanceOf[Throwable]\n      val writer = new StringWriter()\n      val out = new PrintWriter(writer)\n      throwable.printStackTrace(out)\n\n      mutable.Map(\n        ResNames.VALUE -> writer.toString\n      )\n    }\n  }\n\n  class HandlerManager(enableProfiling: Boolean,\n                       timeout: Int,\n                       stringSizeLimit: Int,\n                       collectionSizeLimit: Int,\n                       referenceManager: ReferenceManager) {\n    private val handlerChain = ListBuffer[AbstractTypeHandler](\n      new NullHandler(),\n      new StringHandler(stringSizeLimit),\n      new ArrayHandler(collectionSizeLimit, timeout),\n      new JavaCollectionHandler(collectionSizeLimit, timeout),\n      new SeqHandler(collectionSizeLimit, timeout),\n      new SetHandler(collectionSizeLimit, timeout),\n      new MapHandler(collectionSizeLimit, timeout),\n      new ThrowableHandler(),\n      new SpecialsHandler(stringSizeLimit),\n      new PrimitiveHandler(),\n      new DatasetHandler(),\n      new RDDHandler(),\n      new SparkContextHandler(),\n      new SparkSessionHandler(),\n      new ObjectHandler(stringSizeLimit, this, referenceManager, timeout)\n    ).map(new HandlerWrapper(_, enableProfiling))\n\n    def getErrors: mutable.Seq[String] = handlerChain.flatMap(x => x.handler.getErrors)\n\n    def handleVariable(info: ScalaVariableInfo, loopback: Loopback, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      handlerChain.find(_.accept(info)).map(_.handle(info, loopback, depth, startTime)).getOrElse(mutable.Map[String, Any]())\n    }\n  }\n\n  class HandlerWrapper(val handler: TypeHandler, profile: Boolean) {\n    def accept(info: ScalaVariableInfo): Boolean = info.isLazy || handler.accept(info.value)\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int, initStartTime: Long): Any = {\n      val startTime = if (initStartTime != null)\n        initStartTime\n      else\n        System.currentTimeMillis()\n\n      val data = if (scalaInfo.isLazy) {\n        mutable.Map[String, Any](ResNames.LAZY -> true)\n      }\n      else {\n        try {\n          val data = handler.handle(scalaInfo, loopback, depth: Int)\n          if (data.keys.count(_ == ResNames.IS_PRIMITIVE) > 0) {\n            return data(ResNames.VALUE)\n          }\n          data\n        } catch {\n          case t: Throwable =>\n            return ExceptionUtils.getRootCauseMessage(t)\n        }\n\n      }\n      try {\n        data.put(ResNames.TYPE, calculateType(scalaInfo))\n      } catch {\n        case t: Throwable =>\n          data.put(ResNames.TYPE, ExceptionUtils.getRootCauseMessage(t))\n      }\n\n      if (profile)\n        data.put(ResNames.TIME, System.currentTimeMillis() - startTime)\n\n      data\n    }\n\n    private def calculateType(scalaInfo: ScalaVariableInfo): String = {\n      if (scalaInfo.tpe != null)\n        return scalaInfo.tpe\n\n      if (scalaInfo.value != null)\n        scalaInfo.value.getClass.getCanonicalName\n      else\n        null\n    }\n  }\n  class InterpreterHandler(val interpreter: IMain) {\n    val wrapper = new ZtoolsInterpreterWrapper(interpreter)\n\n    def getVariableNames: immutable.Seq[String] =\n      interpreter.definedSymbolList.filter { x => x.isGetter }.map(_.name.toString).distinct\n\n    def getInfo(name: String, tpe: String): ScalaVariableInfo = {\n      val obj = valueOfTerm(name).orNull\n      ScalaVariableInfo(isAccessible = true, isLazy = false, obj, tpe, name, null)\n    }\n\n    def valueOfTerm(id: String): Option[Any] = wrapper.valueOfTerm(id)\n  }\n\n  case class ScalaVariableInfo(isAccessible: Boolean,\n                               isLazy: Boolean,\n                               value: Any,\n                               tpe: String,\n                               path: String,\n                               ref: String) {\n    val name: String = if (path != null)\n      path.substring(path.lastIndexOf('.') + 1)\n    else\n      null\n  }\n\n\n\n  //noinspection TypeAnnotation\n  class ZtoolsInterpreterWrapper(val iMain: IMain) {\n\n    import scala.language.implicitConversions\n    import scala.reflect.runtime.{universe => ru}\n    import iMain.global._\n\n    import scala.util.{Try => Trying}\n\n    private lazy val importToGlobal = iMain.global mkImporter ru\n    private lazy val importToRuntime = ru.internal createImporter iMain.global\n\n    private implicit def importFromRu(sym: ru.Symbol) = importToGlobal importSymbol sym\n\n    private implicit def importToRu(sym: Symbol): ru.Symbol = importToRuntime importSymbol sym\n\n    // see https://github.com/scala/scala/pull/5852/commits/a9424205121f450dea2fe2aa281dd400a579a2b7\n    def valueOfTerm(id: String): Option[Any] = exitingTyper {\n      def fixClassBasedFullName(fullName: List[String]): List[String] = {\n        if (settings.Yreplclassbased.value) {\n          val line :: read :: rest = fullName\n          line :: read :: \"INSTANCE\" :: rest\n        } else fullName\n      }\n\n      def value(fullName: String) = {\n        val universe = iMain.runtimeMirror.universe\n        import universe.{InstanceMirror, Symbol, TermName}\n        val pkg :: rest = fixClassBasedFullName((fullName split '.').toList)\n        val top = iMain.runtimeMirror.staticPackage(pkg)\n\n        @annotation.tailrec\n        def loop(inst: InstanceMirror, cur: Symbol, path: List[String]): Option[Any] = {\n          def mirrored =\n            if (inst != null) inst\n            else iMain.runtimeMirror reflect (iMain.runtimeMirror reflectModule cur.asModule).instance\n\n          path match {\n            case last :: Nil =>\n              cur.typeSignature.decls find (x => x.name.toString == last && x.isAccessor) map { m =>\n                (mirrored reflectMethod m.asMethod).apply()\n              }\n            case next :: rest =>\n              val s = cur.typeSignature.member(TermName(next))\n              val i =\n                if (s.isModule) {\n                  if (inst == null) null\n                  else iMain.runtimeMirror reflect (inst reflectModule s.asModule).instance\n                }\n                else if (s.isAccessor) {\n                  iMain.runtimeMirror reflect (mirrored reflectMethod s.asMethod).apply()\n                }\n                else {\n                  assert(false, s.fullName)\n                  inst\n                }\n              loop(i, s, rest)\n            case Nil => None\n          }\n        }\n\n        loop(null, top, rest)\n      }\n\n      Option(iMain.symbolOfTerm(id)) filter (_.exists) flatMap (s => Trying(value(s.fullName)).toOption.flatten)\n    }\n  }\n\n  class ReferenceManager {\n    private val refMap = mutable.Map[ReferenceWrapper, String]()\n    private val refInvMap = new TrieMap[ReferenceWrapper]()\n\n    /**\n     * Returns a reference (e.g. valid path) to the object or creates a record in reference maps (and returns null).\n     *\n     * @param obj  an object we want to find a reference for (can be null)\n     * @param path path of the object e.g. myVar.myField.b\n     * @return reference path to the object obj. The method returns null if obj is null itself or\n     *         obj hasn't been mentioned earlier or in the case of AnyVal object.\n     */\n    def getRef(obj: Any, path: String): String = obj match {\n      case null | _: Unit =>\n        clearRefIfPathExists(path)\n        null\n      case ref: AnyRef =>\n        val wrapper = new ReferenceWrapper(ref)\n        if (refMap.contains(wrapper)) {\n          if (refInvMap.get(path).orNull != wrapper) clearRefIfPathExists(path)\n          refMap(wrapper)\n        } else {\n          clearRefIfPathExists(path)\n          refMap(wrapper) = path\n          refInvMap.put(path, wrapper)\n          null\n        }\n      case _ => null\n    }\n\n\n    private def clearRefIfPathExists(path: String): Unit = {\n      if (refInvMap.contains(path)) {\n        val tree = refInvMap.subtree(path)\n        tree.forEach(refMap.remove(_: ReferenceWrapper))\n      }\n    }\n  }\n\n  class ReferenceWrapper(val ref: AnyRef) {\n    override def hashCode(): Int = ref.hashCode()\n\n    override def equals(obj: Any): Boolean = obj match {\n      case value: ReferenceWrapper =>\n        ref.eq(value.ref)\n      case _ => false\n    }\n  }\n\n\n  class VariablesView(val intp: IMain,\n                      val timeout: Int,\n                      val variableTimeout: Int,\n                      val collectionSizeLimit: Int,\n                      val stringSizeLimit: Int,\n                      val blackList: List[String],\n                      val whiteList: List[String] = null,\n                      val filterUnitResults: Boolean,\n                      val enableProfiling: Boolean,\n                      val depth: Int,\n                      val interpreterResCountLimit: Int = 5) {\n    val errors: mutable.MutableList[String] = mutable.MutableList[String]()\n    private val interpreterHandler = new InterpreterHandler(intp)\n    private val referenceManager = new ReferenceManager()\n\n    private val touched = mutable.Map[String, ScalaVariableInfo]()\n\n    private val handlerManager = new HandlerManager(\n      collectionSizeLimit = collectionSizeLimit,\n      stringSizeLimit = stringSizeLimit,\n      timeout = variableTimeout,\n      referenceManager = referenceManager,\n      enableProfiling = enableProfiling\n    )\n\n    //noinspection ScalaUnusedSymbol\n    def getZtoolsJsonResult: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(\n        Map(\n          \"variables\" -> resolveVariables,\n          \"errors\" -> (errors ++ handlerManager.getErrors)\n        )\n      )\n    }\n\n    def toJson: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(resolveVariables)\n    }\n\n    def resolveVariables: mutable.Map[String, Any] = {\n      val result: mutable.Map[String, Any] = mutable.Map[String, Any]()\n      val startTime = System.currentTimeMillis()\n\n      val interpreterVariablesNames = interpreterHandler.getVariableNames\n      val finalNames = filterVariableNames(interpreterVariablesNames)\n\n      finalNames.foreach { name =>\n        val varType = interpreterHandler.interpreter.typeOfTerm(name).toString().stripPrefix(\"()\")\n        val variable = mutable.Map[String, Any]()\n\n        result += name -> variable\n        variable += ResNames.TYPE -> varType\n        if (!isUnitOrNullResult(result, name))\n          variable += ResNames.VALUE -> \"<Not calculated>\"\n      }\n\n      var passedVariablesCount = 0\n      val totalVariablesCount = finalNames.size\n\n      if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n        return result\n\n      finalNames.foreach { name =>\n        if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n          return result\n        passedVariablesCount += 1\n\n        if (!isUnitOrNullResult(result, name)) {\n\n          calculateVariable(result, name)\n        }\n      }\n      result\n    }\n\n    private def calculateVariable(result: mutable.Map[String, Any], name: String) = {\n      val valMap = result(name).asInstanceOf[mutable.Map[String, Any]]\n      try {\n        val startTime = System.currentTimeMillis()\n\n        val info = interpreterHandler.getInfo(name, valMap(ResNames.TYPE).asInstanceOf[String])\n        val ref = referenceManager.getRef(info.value, name)\n        touched(info.path) = info\n\n        if (ref != null && ref != info.path) {\n          result += (info.path -> mutable.Map[String, Any](ResNames.REF -> ref))\n        } else {\n          result += info.path -> parseInfo(info, depth, startTime)\n        }\n      } catch {\n        case t: Throwable =>\n          valMap += ResNames.VALUE -> ExceptionUtils.getRootCauseMessage(t)\n      }\n    }\n\n    private def isUnitOrNullResult(result: mutable.Map[String, Any], name: String) = {\n      val res = result(name).asInstanceOf[mutable.Map[String, Any]]\n      val valType = res(ResNames.TYPE)\n      valType == \"Unit\" || valType == \"Null\"\n    }\n\n    def resolveVariable(path: String): mutable.Map[String, Any] = {\n      val result = mutable.Map[String, Any]()\n      val obj = touched.get(path).orNull\n      if (obj.ref != null) {\n        result += (ResNames.VALUE -> mutable.Map[String, Any](ResNames.REF -> obj.ref))\n      } else {\n        result += (ResNames.VALUE -> parseInfo(obj, depth))\n      }\n      result\n    }\n\n    private def parseInfo(info: ScalaVariableInfo, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      val loopback = new Loopback {\n        override def pass(obj: Any, id: String): Any = {\n          val si = ScalaVariableInfo(isAccessible = true, isLazy = false, obj, null, id, referenceManager.getRef(obj, id))\n          parseInfo(si, depth - 1)\n        }\n      }\n      handlerManager.handleVariable(info, loopback, depth, startTime)\n    }\n\n    private def filterVariableNames(interpreterVariablesNames: Seq[String]) = {\n      val variablesNames = interpreterVariablesNames.seq\n        .filter { name => !blackList.contains(name) }\n        .filter { name => whiteList == null || whiteList.contains(name) }\n\n\n      val p = Pattern.compile(\"res\\\\d*\")\n      val (resVariables, otherVariables: immutable.Seq[String]) = variablesNames.partition(x => p.matcher(x).matches())\n      val sortedResVariables = resVariables\n        .map(res => Try(res.stripPrefix(\"res\").toInt))\n        .filter(_.isSuccess)\n        .map(_.get)\n        .sortWith(_ > _)\n        .take(interpreterResCountLimit)\n        .map(num => \"res\" + num)\n\n      val finalNames = otherVariables ++ sortedResVariables\n      finalNames\n    }\n\n    //noinspection ScalaUnusedSymbol\n    private implicit def toJavaFunction[A, B](f: A => B): JFunction[A, B] = new JFunction[A, B] {\n      override def apply(a: A): B = f(a)\n    }\n\n    private def checkTimeout(startTimeout: Long, passed: Int, total: Int): Boolean = {\n      val isTimeoutExceed = System.currentTimeMillis() - startTimeout > timeout\n      if (isTimeoutExceed)\n        errors += s\"Variables collect timeout. Exceed ${timeout}ms. Parsed $passed from $total.\"\n      isTimeoutExceed\n    }\n  }\n\n  class DatasetHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Dataset[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val df = obj.asInstanceOf[Dataset[_]]\n\n\n      val schema = df.schema\n      val jsonSchemaColumns = schema.fields.map(field => {\n        val value = withJsonObject { jsonField =>\n          jsonField += \"name\" -> wrap(field.name, null)\n          jsonField += \"nullable\" -> wrap(field.nullable, null)\n          jsonField += \"dataType\" -> wrap(field.dataType.typeName, null)\n        }\n        wrap(value, \"org.apache.spark.sql.types.StructField\")\n      }\n      )\n\n      val jsonSchema = mutable.Map(\n        ResNames.VALUE -> jsonSchemaColumns,\n        ResNames.TYPE -> \"org.apache.spark.sql.types.StructType\",\n        ResNames.LENGTH -> jsonSchemaColumns.length\n      )\n\n      val dfValue = mutable.Map(\n        \"schema()\" -> jsonSchema,\n        \"getStorageLevel()\" -> wrap(df.storageLevel.toString(), \"org.apache.spark.storage.StorageLevel\")\n      )\n\n      mutable.Map(\n        ResNames.VALUE -> dfValue\n      )\n    }\n  }\n\n\n  class RDDHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[RDD[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val rdd = obj.asInstanceOf[RDD[_]]\n        json += (ResNames.VALUE -> withJsonObject { value =>\n          value += (\"getNumPartitions()\" -> wrap(rdd.getNumPartitions, \"Int\"))\n          value += (\"name\" -> wrap(rdd.name, \"String\"))\n          value += (\"id\" -> wrap(rdd.id, \"Int\"))\n          value += (\"partitioner\" -> wrap(rdd.partitioner.toString, \"Option[org.apache.spark.Partitioner]\"))\n          value += (\"getStorageLevel()\" -> wrap(rdd.getStorageLevel.toString, \"org.apache.spark.storage.StorageLevel\"))\n        })\n    }\n  }\n\n  class SparkContextHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkContext]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val sc = scalaInfo.value.asInstanceOf[SparkContext]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"sparkUser\" -> wrap(sc.sparkUser, \"String\"))\n          json += (\"sparkTime\" -> wrap(sc.startTime, \"Long\"))\n          json += (\"applicationId()\" -> wrap(sc.applicationId, \"String\"))\n          json += (\"applicationAttemptId()\" -> wrap(sc.applicationAttemptId.toString, \"Option[String]\"))\n          json += (\"appName()\" -> sc.appName)\n        })\n    }\n  }\n\n  class SparkSessionHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkSession]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val id = scalaInfo.path\n\n        val spark = obj.asInstanceOf[SparkSession]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"version()\" -> spark.version)\n          json += (\"sparkContext\" -> loopback.pass(spark.sparkContext, s\"$id.sparkContext\"))\n        })\n    }\n  }\n\n\n  /**\n   * Main section\n   */\n  val iMain: IMain = $intp\n  val depth: Int = 2\n  val filterUnitResults: Boolean = true\n  val enableProfiling: Boolean = true\n  val collectionSizeLimit = 100\n  val stringSizeLimit = 400\n  val timeout = 5000\n  val variableTimeout = 2000\n  val interpreterResCountLimit = 10\n  val blackList = \"$intp,sqlContext,z,engine\".split(',').toList\n  val whiteList: List[String] =  null\n\n\n  val variableView = new VariablesView(\n    intp = iMain,\n    timeout = timeout,\n    variableTimeout = variableTimeout,\n    collectionSizeLimit = collectionSizeLimit,\n    stringSizeLimit = stringSizeLimit,\n    blackList = blackList,\n    whiteList = whiteList,\n    filterUnitResults = filterUnitResults,\n    enableProfiling = enableProfiling,\n    depth = depth,\n    interpreterResCountLimit = interpreterResCountLimit\n  )\n\n  implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n  val variablesJson = variableView.getZtoolsJsonResult\n  println(\"---ztools-scala---\")\n  println(variablesJson)\n  println(\"---ztools-scala---\")\n}\ncatch {\n  case t: Throwable =>\n    import org.apache.commons.lang.exception.ExceptionUtils\n    import org.json4s.jackson.Serialization\n    import org.json4s.{Formats, NoTypeHints}\n\n    implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n    val result = Serialization.write(Map(\n      \"errors\" -> Array(f\"${ExceptionUtils.getMessage(t)}\\n${ExceptionUtils.getStackTrace(t)}\")\n    ))\n    println(\"---ztools-scala---\")\n    println(result)\n    println(\"---ztools-scala---\")\n}\n{\n    var sqlTableShows: Array[String] = null\n    val additionalTables = Array[Tuple2[String, String]]((\"\", \"hotel\"), (\"\", \"tipologias\"), (\"\", \"quartos_reservados\"), (\"\", \"feriados\"))\n    val timeout = 5000\n    val collectOnlyTempTables = false\n    val appendOutput = true\n\n    case class ZtoolsColumn(name: String,\n                            columnType: String,\n                            description: String)\n\n    case class ZtoolsTable(name: String,\n                           databaseName: String,\n                           var columns: Array[ZtoolsColumn],\n                           var error: String = null)\n\n    case class ZtoolsSqlProfile(request: String, time: Long)\n\n    case class ZtoolsSqlInfo(tables: Array[ZtoolsTable],\n                             errors: Array[String],\n                             profiling: Array[ZtoolsSqlProfile],\n                             appendOutput: Boolean = appendOutput)\n\n\n    //TO KNOW:\n    //We collect info by spark.sql not spark.catalog because there some errors with Glue, database does not read\n    //Additionally we cannot use column name because it can be different \"namespace\" in EMR and \"database\" in vanilla spark\n    def calcZtoolsSqlSchemas(): String = {\n        import com.fasterxml.jackson.annotation.JsonAutoDetect.Visibility\n        import com.fasterxml.jackson.annotation.PropertyAccessor\n        import com.fasterxml.jackson.databind.ObjectMapper\n        import org.apache.commons.lang.exception.ExceptionUtils\n        import org.apache.spark.sql.Row\n\n        import scala.collection.mutable.ArrayBuffer\n\n        val startTime = System.currentTimeMillis()\n        val errors = ArrayBuffer[String]()\n\n        def convertThrowable(msg: String, t: Throwable): String = msg + \"\\n\" +\n                ExceptionUtils.getRootCauseMessage(t) + \"\\n\" +\n                ExceptionUtils.getStackTrace(t)\n\n        def escapeSql(string: String) = \"`\" + string.replace(\"`\", \"``\") + \"`\"\n\n\n\n\n        var tables = ArrayBuffer[ZtoolsTable]()\n        var profilingResult = ArrayBuffer[ZtoolsSqlProfile]()\n\n        def performSql(sqlRequest: String): Tuple2[Array[_ <: Row], String] = {\n            if (System.currentTimeMillis() - startTime > timeout) {\n                val error = f\"Timeout $timeout exceed. Sql request '$sqlRequest' ignored.\"\n                errors.append(error)\n                return (Array.empty, error)\n            }\n            val startTransactionTime = System.currentTimeMillis()\n            try {\n                val rows = spark.sql(sqlRequest).collect()\n                (rows, null)\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(sqlRequest, t))\n                    (Array.empty, ExceptionUtils.getMessage(t))\n            } finally {\n                profilingResult += ZtoolsSqlProfile(sqlRequest, System.currentTimeMillis() - startTransactionTime)\n            }\n        }\n\n        if (sqlTableShows!=null && sqlTableShows.isEmpty) {\n            val sqlRequest = \"show databases\"\n            val databases = performSql(sqlRequest)._1.map(_.getAs[String](0))\n            sqlTableShows = databases.map(db => f\"SHOW TABLES in $db\")\n        }\n\n        if (sqlTableShows==null) {\n            sqlTableShows = Array.empty\n        }\n\n        sqlTableShows.foreach(sqlRequest => {\n            try {\n                var listTables = performSql(sqlRequest)._1\n                if (collectOnlyTempTables)\n                    listTables = listTables.filter(_.getAs[Boolean](2) == true)\n\n                listTables.map(row => ZtoolsTable(\n                    databaseName = row.getAs[String](0),\n                    name = row.getAs[String](1),\n                    columns = Array.empty[ZtoolsColumn])).foreach(t => tables.append(t))\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(s\"Error transform output of  $sqlRequest\", t))\n                    ArrayBuffer.empty[ZtoolsTable]\n            }\n        })\n\n        val tableSet = (additionalTables.map(it => ZtoolsTable(it._2, it._1, Array.empty)) ++ tables).distinct\n\n        def processTable(table: ZtoolsTable): Unit = {\n            val columns = try {\n                val tableSqlName = if (table.databaseName == null || table.databaseName.isEmpty)\n                    escapeSql(table.name)\n                else\n                    escapeSql(table.databaseName) + \".\" + escapeSql(table.name)\n\n                //https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-describe-table.html\n                val sqlResult = performSql(s\"DESCRIBE TABLE $tableSqlName\")\n\n                val columnRows = sqlResult._1\n                table.error = sqlResult._2\n\n                //Ignore partition section\n                columnRows.takeWhile(row => !Option(row.getAs[String](0)).getOrElse(\"\").startsWith(\"# \"))\n                        .map(row => ZtoolsColumn(row.getAs[String](0), row.getAs[String](1), row.getAs[String](2)))\n            } catch {\n                case t: Throwable => convertThrowable(s\"Error list columns for ${table.name}\", t)\n                    table.error = ExceptionUtils.getRootCauseMessage(t)\n                    errors.append(convertThrowable(s\"Error list columns for ${table.name}\", t))\n                    return\n            }\n            table.columns = columns\n        }\n\n        tableSet.foreach(table => {\n            processTable(table)\n        })\n\n        val res = ZtoolsSqlInfo(tableSet.toArray, errors.toArray, profilingResult.toArray)\n        val objectMapper = new ObjectMapper().setVisibility(PropertyAccessor.FIELD, Visibility.ANY).writerWithDefaultPrettyPrinter()\n        objectMapper.writeValueAsString(res)\n    }\n\n    def ztoolsPrintResult(): Unit = {\n        val ztoolsSqlResult = calcZtoolsSqlSchemas()\n        println(\"---ztools-sql---\")\n        println(ztoolsSqlResult)\n        println(\"---ztools-sql---\")\n    }\n\n    ztoolsPrintResult()\n}",
   "id": "",
   "dateCreated": "2023-04-29 17:01:41.160",
   "config": {
    "tableHide": true,
    "editorHide": true
   },
   "dateStarted": "2023-04-29 17:01:42.919",
   "dateUpdated": "2023-04-29 17:01:42.919"
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {},
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "import org.apache.spark.sql.functions._\n//Leitura de dados Facilities\nval csvFacilities = \"/data/tp/Facilities.csv\"\n\n// Leitura csv\nval df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\") // set this to true if your CSV file has header\n  .option(\"inferSchema\", \"true\") // set this to true if you want Spark to infer the schema\n  .option(\"delimiter\", \";\") // ; is the separator\n  .load(csvFacilities)\n\nval atributos = df.columns\nprintln(\"Atributos:\")\natributos.foreach(println)\n//val summary_facilities = df.describe()\n//summary_facilities.show()\n/*    summary_facilities.coalesce(1).write\n .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"delimiter\", \",\")\n    .mode(\"overwrite\")\n    .save(\"/data/tp/summary_facilities.csv\")\n*/\n//df.printSchema()\n//df.show()\n",
   "id": "",
   "dateCreated": "2023-04-23 23:44:45.859",
   "config": {
    "tableHide": false
   },
   "dateStarted": "2023-04-29 17:06:57.253",
   "dateUpdated": "2023-04-29 17:06:57.655",
   "dateFinished": "2023-04-29 17:06:57.655",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "Atributos:\nHotel ID\nFacility ID\nNome\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mcsvFacilities\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Facilities.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [Hotel ID: int, Facility ID: int ... 1 more field]\n\u001b[1m\u001b[34matributos\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(Hotel ID, Facility ID, Nome)\n"
     }
    ]
   }
  },
  {
   "user": "anonymous",
   "config": {
    "colWidth": 12,
    "fontSize": 9,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {},
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "jobName": "paragraph_1563110258183_1613653816",
   "id": "20190714-161738_1950435706",
   "dateCreated": "2019-07-14T16:17:38+0300",
   "status": "FINISHED",
   "progressUpdateIntervalMs": 500,
   "focus": true,
   "$$hashKey": "object:394",
   "text": "import org.apache.spark.sql.functions._\n//Leitura de dados Hotel\nval csvHotel = \"/data/tp/Hotel.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvHotel)\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"Localização\", \"localizacao\")\n        .withColumnRenamed(\"Estrelas\", \"estrelas\")\n        .withColumnRenamed(\"Idade Máxima de Crianças (Anos)\", \"idade_max_criancas\")\n        .withColumnRenamed(\"Idade Máxima de Bebés (Meses)\", \"idade_max_bebes\")\n        .withColumnRenamed(\"Hora máxima de check-in\", \"hora_max_checkin\")\n        .withColumnRenamed(\"Quantidade de quartos\", \"qtd_quartos\")\n\n//val colums = df.columns\n//println(\"colums:\")\n//colums.foreach(println)\ndf.createOrReplaceTempView(\"Hotel\")\ndf.cache();\ndf.describe().show()\n//df.show()\n\n",
   "dateStarted": "2023-04-29 17:07:29.167",
   "dateUpdated": "2023-04-29 17:07:32.176",
   "dateFinished": "2023-04-29 17:07:32.175",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+------------------+-----------+------------------+------------------+------------------+----------------+------------------+\n|summary|          hotel_ID|localizacao|          estrelas|idade_max_criancas|   idade_max_bebes|hora_max_checkin|       qtd_quartos|\n+-------+------------------+-----------+------------------+------------------+------------------+----------------+------------------+\n|  count|               145|        145|               145|               145|               145|             145|               145|\n|   mean| 395.7586206896552|       null|1.7862068965517242| 7.324137931034483|20.020689655172415|            null|30.020689655172415|\n| stddev|118.24939956754241|       null| 1.780239555105778| 6.145100531024569|20.013699809795995|            null|33.797080118274614|\n|    min|                20|          .|                 0|                 0|                 0|00:01:00.0000000|                 1|\n|    max|               561|      teste|                 5|                36|               168|23:59:00.0000000|               192|\n+-------+------------------+-----------+------------------+------------------+------------------+----------------+------------------+\n\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mcsvHotel\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Hotel.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hotel_ID: int, localizacao: string ... 5 more fields]\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {}
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados Tipologias\nval csvTipologias = \"/data/tp/Tipologias.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvTipologias)\n        .withColumnRenamed(\"Room ID\", \"room_ID\")\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"Tipo de quarto\", \"tipo_quarto\")\n        .withColumnRenamed(\"Quantidade\", \"quantidade\")\n        .withColumnRenamed(\"Capacidade máxima\", \"capacidade_maxima\")\n        .withColumnRenamed(\"Capacidade máxima de adultos\", \"capacidade_max_adultos\")\n        .withColumnRenamed(\"Capacidade máxima de crianças\", \"capacidade_max_criancas\")\n        .withColumnRenamed(\"Capacidade máxima de bebés\", \"capacidade_max_bebes\")\n\ndf.select(\n    col(\"tipo_quarto\"),\n    col(\"quantidade\"),\n    col(\"capacidade_maxima\"),\n    col(\"capacidade_max_adultos\"),\n    col(\"capacidade_max_criancas\"),\n    col(\"capacidade_max_bebes\")\n  ).describe().show()\ndf.createOrReplaceTempView(\"Tipologias\")\ndf.cache();\n//df.show()\n",
   "id": "",
   "dateCreated": "2023-04-24 21:16:30.526",
   "config": {},
   "dateStarted": "2023-04-29 15:20:46.985",
   "dateUpdated": "2023-04-29 15:20:47.454",
   "dateFinished": "2023-04-29 15:20:47.454"
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados Quartos Reservados\nval csvQ_Reservados= \"/data/tp/Quartos_Reservados.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvQ_Reservados)\n        .withColumnRenamed(\"Reserve ID\", \"Reserve_ID\")\n        .withColumnRenamed(\"Room ID\", \"room_ID\")\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"País\", \"pais\")\n        .withColumnRenamed(\"Estado da reserva\", \"estado_reserva\")\n        .withColumnRenamed(\"RatePlan\", \"rate_plan\")\n        .withColumnRenamed(\"Data da reserva\", \"data_reserva\")\n        .withColumnRenamed(\"Data chegada\", \"data_chegada\")\n        .withColumnRenamed(\"Data de partida\", \"data_partida\")\n        .withColumnRenamed(\"Número de noites\", \"num_noites\")\n        .withColumnRenamed(\"Ocupação\", \"ocupacao\")\n        .withColumnRenamed(\"Adultos\", \"adultos\")\n        .withColumnRenamed(\"Crianças\", \"criancas\")\n        .withColumnRenamed(\"Preço (€)\", \"preco_euros\")\n\ndf.createOrReplaceTempView(\"Quartos_Reservados\")\ndf.cache()\ndf.show()\n\ndf.select(\"pais\", \"estado_reserva\", \"rate_plan\", \"data_reserva\", \"data_chegada\", \"data_partida\", \"num_noites\", \"ocupacao\", \"adultos\", \"criancas\", \"preco_euros\").describe().show()\n",
   "id": "",
   "dateCreated": "2023-04-24 21:18:31.653",
   "config": {},
   "dateStarted": "2023-04-29 23:01:18.198",
   "dateUpdated": "2023-04-29 23:01:22.458",
   "dateFinished": "2023-04-29 23:01:22.458",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+--------+----------+--------------+--------------+-------+--------------------+--------------------+--------------------+------------+------------+----------+--------+-------+--------+-----+-----------+\n|hotel_ID|Reserve_ID|          pais|estado_reserva|room_ID|      Tipo de Quarto|           rate_plan|        data_reserva|data_chegada|data_partida|num_noites|ocupacao|adultos|criancas|Bebés|preco_euros|\n+--------+----------+--------------+--------------+-------+--------------------+--------------------+--------------------+------------+------------+----------+--------+-------+--------+-----+-----------+\n|     311|   1418210|       Espanha|     Registado|   1407|DB - Quarto Duplo...|    DB Room Only NRF|2022-01-01 10:17:...|  02/01/2022|  03/01/2022|         1|       2|      2|       0|    2|       63.0|\n|     309|   1418228|      Portugal|    Modificada|   1372|Quarto Duplo Stan...|           DB RO NRF|2022-01-01 12:19:...|  01/01/2022|  02/01/2022|         1|       2|      2|       0|    2|       38.7|\n|     395|   1418293|         Suiça|     Registado|   1829|     Estúdio Jardim |            Standard|2022-01-01 17:34:...|  16/08/2022|  18/08/2022|         2|       2|      4|       0|    2|      500.0|\n|     283|   1418305|       Holanda|     Registado|   1241|             Estudio|            Standard|2022-01-01 18:30:...|  02/01/2022|  05/01/2022|         3|       1|      1|       0|    1|      126.0|\n|     444|   1418314|Estados Unidos|     Registado|   2074|Duplo ou Twin  Vi...|              Normal|2022-01-01 19:00:...|  07/01/2022|  08/01/2022|         1|       1|      1|       0|    1|       80.0|\n|     535|   1418316|      Portugal|     Registado|   2783|            Bungalow|             WebSite|2022-01-01 19:23:...|  04/01/2022|  06/01/2022|         2|       2|      2|       0|    2|      135.0|\n|     539|   1418318|      Portugal|     Registado|   2542|Quarto Dpl. Standard|                 Bar|2022-01-01 19:27:...|  02/01/2022|  03/01/2022|         1|       1|      1|       0|    1|       30.0|\n|     535|   1418332|      Portugal|     Registado|   2526|     Quarto Familiar|             WebSite|2022-01-01 20:46:...|  02/01/2022|  04/01/2022|         2|       2|      2|       0|    2|      162.0|\n|     414|   1418334|      Portugal|     Registado|   2049|        Suite Junior|        Normal - Bar|2022-01-01 21:06:...|  02/01/2022|  03/01/2022|         1|       2|      2|       0|    2|      100.0|\n|     556|   1418346|      Portugal|     Registado|   2831|Double Room Standard|NRF - Non Refundable|2022-01-01 21:51:...|  06/01/2022|  08/01/2022|         2|       2|      2|       0|    2|      105.6|\n|     556|   1418346|      Portugal|     Registado|   2833|Triple Room Standard|NRF - Non Refundable|2022-01-01 21:51:...|  06/01/2022|  08/01/2022|         2|       3|      3|       0|    3|      158.4|\n|     285|   1418404|       Bélgica|     Registado|   1288|              STUDIO|                  7N|2022-01-02 11:09:...|  12/05/2022|  21/05/2022|         9|       2|      2|       0|    2|      367.2|\n|     535|   1418455|      Portugal|      Pendente|   2523|        Quarto Duplo|             WebSite|2022-01-02 14:08:...|  02/01/2022|  03/01/2022|         1|       2|      2|       0|    2|       58.5|\n|     556|   1418531|      Portugal|     Registado|   2831|Double Room Standard|WEB (Best Availab...|2022-01-02 17:55:...|  03/01/2022|  05/01/2022|         2|       2|      2|       0|    2|      114.0|\n|     535|   1418540|      Portugal|      Pendente|   2523|        Quarto Duplo|             WebSite|2022-01-02 18:36:...|  05/01/2022|  06/01/2022|         1|       2|      2|       0|    2|       58.5|\n|     444|   1418544|      Portugal|     Registado|   2074|Duplo ou Twin  Vi...|              Normal|2022-01-02 18:57:...|  13/01/2022|  15/01/2022|         2|       2|      2|       0|    2|      160.0|\n|     556|   1418568|      Portugal|     Cancelado|   2831|Double Room Standard|WEB (Best Availab...|2022-01-02 19:54:...|  15/01/2022|  16/01/2022|         1|       2|      2|       0|    2|       76.0|\n|     535|   1418570|      Portugal|      Pendente|   2523|        Quarto Duplo|             WebSite|2022-01-02 19:58:...|  07/01/2022|  08/01/2022|         1|       2|      2|       0|    2|       58.5|\n|     539|   1418582|      Portugal|     Registado|   2542|Quarto Dpl. Standard|                 Bar|2022-01-02 20:57:...|  18/02/2022|  20/02/2022|         2|       1|      1|       0|    1|        0.0|\n|     556|   1418600|      Portugal|      Pendente|   2831|Double Room Standard|WEB (Best Availab...|2022-01-02 22:04:...|  08/01/2022|  09/01/2022|         1|       2|      6|       0|    2|      228.0|\n+--------+----------+--------------+--------------+-------+--------------------+--------------------+--------------------+------------+------------+----------+--------+-------+--------+-----+-----------+\nonly showing top 20 rows\n\n+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n|summary|   pais|estado_reserva|           rate_plan|data_chegada|data_partida|       num_noites|          ocupacao|           adultos|           criancas|       preco_euros|\n+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n|  count|  25105|         25105|               25105|       25105|       25105|            25105|             25105|             25105|              25105|             25105|\n|   mean|   null|          null|                null|        null|        null|2.282015534754033|1.8697868950408285| 2.041625174268074| 0.0614618601872137|243.33035643239006|\n| stddev|   null|          null|                null|        null|        null|4.682108190781085|0.6624708635044815|1.0247398139530792|0.29563994588421044| 373.5384599086867|\n|    min|APO/FPO|     Cancelado| PROGRAMA PÁSCOA ...|  01/01/2022|  01/01/2023|                1|                 1|                 1|                  0|               0.0|\n|    max|Áustria|     Registado|    Winter Promotion|  31/12/2022|  31/12/2023|              481|                 9|                30|                  4|           20683.0|\n+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n\n\u001b[1m\u001b[34mcsvQ_Reservados\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Quartos_Reservados.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hotel_ID: int, Reserve_ID: int ... 14 more fields]\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados externos Feriados\nval csvFeriados= \"/data/tp/Feriados.csv\"\n\nval df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\") // set this to true if your CSV file has header\n  .option(\"inferSchema\", \"true\") // set this to true if you want Spark to infer the schema\n  .option(\"delimiter\", \";\") // ; is the separator\n  .load(csvFeriados)\n    df.createOrReplaceTempView(\"Feriados\")\n    df.cache()\n\ndf.describe().show()\n//df.show()",
   "id": "",
   "dateCreated": "2023-04-24 22:00:30.321",
   "config": {},
   "dateStarted": "2023-04-29 20:23:59.084",
   "dateUpdated": "2023-04-29 20:24:01.241",
   "dateFinished": "2023-04-29 20:24:01.240",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+----------+-----------------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n|summary|      Date|              day|         dayOfWeek|            month|         trimester|              year|          is_holiday|portugueseWeekName|\n+-------+----------+-----------------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n|  count|       730|              730|               730|              730|               730|               730|                 730|               730|\n|   mean|      null|15.72054794520548|               4.0|6.526027397260274|2.5095890410958903|            2022.5|0.038356164383561646|              null|\n| stddev|      null|8.802278027009615|2.0047953485517556| 3.45021529306815| 1.117532602466349|0.5003428180045024| 0.19218627865352142|              null|\n|    min|01/01/2022|                1|                 1|                1|                 1|              2022|                   0|           Domingo|\n|    max|31/12/2023|               31|                 7|               12|                 4|              2023|                   1|            sabado|\n+-------+----------+-----------------+------------------+-----------------+------------------+------------------+--------------------+------------------+\n\n\u001b[1m\u001b[34mcsvFeriados\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Feriados.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [Date: string, day: int ... 6 more fields]\n"
     }
    ]
   }
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}