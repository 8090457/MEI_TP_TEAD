{
 "paragraphs": [
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "%spark.spark\n//ZToolsId = da758b2d-ef0f-4202-bd32-8941efbaf26d\n// It is generated code for integration with Big Data Tools plugin\n// Please DO NOT edit it.\ntry {\n  import org.apache.commons.lang.exception.ExceptionUtils\n  import org.apache.spark.sql.SparkSession\n\n  import java.io.{PrintWriter, StringWriter}\n  import java.util\n  import scala.collection.mutable.ListBuffer\n  import scala.collection.{immutable, mutable}\n  import scala.reflect.api.JavaUniverse\n  import scala.tools.nsc.interpreter.IMain\n  import org.json4s.jackson.Serialization\n  import org.json4s.{Formats, NoTypeHints}\n\n  import java.util.function.{Function => JFunction}\n  import java.util.regex.Pattern\n  import scala.language.implicitConversions\n  import scala.util.Try\n  import org.apache.spark.sql.Dataset\n  import org.apache.spark.rdd.RDD\n  import org.apache.spark.SparkContext\n\n  trait Loopback {\n    def pass(obj: Any, id: String): Any\n  }\n\n  object ResNames {\n    val REF = \"ref\"\n    val VALUE = \"value\"\n    val IS_PRIMITIVE = \"isPrimitive\"\n    val TYPE = \"type\"\n    val TIME = \"time\"\n    val LENGTH = \"length\"\n    val LAZY = \"lazy\"\n  }\n\n  object TrieMap {\n    class Node[T](var value: Option[T]) {\n      var children: mutable.Map[String, TrieMap.Node[T]] = _\n\n      def put(key: String, node: TrieMap.Node[T]): Option[Node[T]] = {\n        if (children == null)\n          children = mutable.Map[String, TrieMap.Node[T]]()\n        children.put(key, node)\n      }\n\n      def del(key: String): Option[Node[T]] = children.remove(key)\n\n      def forEach(func: Function[T, _]): Unit = {\n        func.apply(value.get)\n        if (children != null) children.foreach(t => t._2.forEach(func))\n      }\n    }\n\n    def split(key: String): Array[String] = {\n      var n = 0\n      var j = 0\n      for (i <- 0 until key.length) {\n        if (key.charAt(i) == '.') n += 1\n      }\n      val k = new Array[String](n + 1)\n      val sb = new mutable.StringBuilder(k.length)\n      for (i <- 0 until key.length) {\n        val ch = key.charAt(i)\n        if (ch == '.') {\n          k({\n            j += 1;\n            j - 1\n          }) = sb.toString\n          sb.setLength(0)\n        }\n        else sb.append(ch)\n      }\n      k(j) = sb.toString\n      k\n    }\n  }\n\n  class TrieMap[T] {\n    val root = new TrieMap.Node[T](null)\n\n    def subtree(key: Array[String], length: Int): TrieMap.Node[T] = {\n      var current = root\n      var i = 0\n      while ( {\n        i < length && current != null\n      }) {\n        if (current.children == null) return null\n        current = current.children.get(key(i)).orNull\n        i += 1\n      }\n      current\n    }\n\n    def put(key: Array[String], value: T): Option[TrieMap.Node[T]] = {\n      val node = subtree(key, key.length - 1)\n      node.put(key(key.length - 1), new TrieMap.Node[T](Option.apply(value)))\n    }\n\n    def put(key: String, value: T): Option[TrieMap.Node[T]] = {\n      val k = TrieMap.split(key)\n      put(k, value)\n    }\n\n    def contains(key: String): Boolean = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      node != null\n    }\n\n    def get(key: String): Option[T] = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      if (node == null) return Option.empty\n      node.value\n    }\n\n    def subtree(key: String): TrieMap.Node[T] = {\n      val k = TrieMap.split(key)\n      subtree(k, k.length)\n    }\n  }\n\n  trait TypeHandler {\n    def accept(obj: Any): Boolean\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any]\n\n    def getErrors: List[String] = List[String]()\n  }\n\n  abstract class AbstractCollectionHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    trait Iterator {\n      def hasNext: Boolean\n\n      def next: Any\n    }\n\n    def iterator(obj: Any): Iterator\n\n    def length(obj: Any): Int\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      if (depth <= 0) {\n        withJsonObject { result =>\n          var s = scalaInfo.value.toString\n          if (s.length>1000)\n            s = s.take(1000) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n      } else {\n        mutable.Map[String, Any](\n          ResNames.LENGTH -> length(scalaInfo.value),\n          ResNames.VALUE -> withJsonArray { json =>\n            val startTime = System.currentTimeMillis()\n            val it = iterator(scalaInfo.value)\n            var index = 0\n            while (it.hasNext && index < limit && !checkTimeoutError(scalaInfo.path, startTime, timeout)) {\n                val id = scalaInfo.path\n                json += loopback.pass(it.next, s\"$id[$index]\")\n                index += 1\n            }\n            })\n      }\n    }\n  }\n\n  abstract class AbstractTypeHandler extends TypeHandler {\n    val timeoutErrors: mutable.MutableList[String] = mutable.MutableList()\n\n    override def getErrors: List[String] = timeoutErrors.toList\n\n    protected def withJsonArray(body: mutable.MutableList[Any] => Unit): mutable.MutableList[Any] = {\n      val arr = mutable.MutableList[Any]()\n      body(arr)\n      arr\n    }\n\n    protected def withJsonObject(body: mutable.Map[String, Any] => Unit): mutable.Map[String, Any] = {\n      val obj = mutable.Map[String, Any]()\n      body(obj)\n      obj\n    }\n\n    protected def wrap(obj: Any, tpe: String): mutable.Map[String, Any] = mutable.Map[String, Any](\n      ResNames.VALUE -> Option(obj).orNull,\n      ResNames.TYPE -> tpe\n    )\n\n    protected def checkTimeoutError(name: String, startTime: Long, timeout: Int): Boolean = {\n      val isTimeout = System.currentTimeMillis() - startTime > timeout\n      if (isTimeout)\n        timeoutErrors += f\"Variable $name collect timeout exceed ${timeout}ms.\"\n      isTimeout\n    }\n\n  }\n\n  class ArrayHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Array[_]]\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Array[_]].length\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Array[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next\n    }\n  }\n\n  class JavaCollectionHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[util.Collection[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator() {\n      private val it = obj.asInstanceOf[util.Collection[_]].iterator()\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[util.Collection[_]].size()\n  }\n  class MapHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject {\n        json =>\n          val obj = scalaInfo.value\n          val id = scalaInfo.path\n          val map = obj.asInstanceOf[Map[_, _]]\n          val keys = mutable.MutableList[Any]()\n          val values = mutable.MutableList[Any]()\n          json += (\"jvm-type\" -> obj.getClass.getCanonicalName)\n          json += (\"length\" -> map.size)\n          var index = 0\n\n          json += (\"key\" -> keys)\n          json += (\"value\" -> values)\n\n          val startTime = System.currentTimeMillis()\n          map.view.take(math.min(limit, map.size)).foreach {\n            case (key, value) =>\n              if (checkTimeoutError(scalaInfo.path, startTime, timeout))\n                return json\n              keys += loopback.pass(key, s\"$id.key[$index]\")\n              values += loopback.pass(value, s\"$id.value[$index]\")\n              index += 1\n          }\n      }\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Map[_, _]]\n  }\n\n  class NullHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj == null\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any]()\n  }\n\n  class ObjectHandler(val stringSizeLimit: Int,\n                      val manager: HandlerManager,\n                      val referenceManager: ReferenceManager,\n                      val timeout: Int) extends AbstractTypeHandler {\n    private val INACCESSIBLE = ScalaVariableInfo(isAccessible = false, isLazy = false, null, null, null, null)\n    val ru: JavaUniverse = scala.reflect.runtime.universe\n    val mirror: ru.Mirror = ru.runtimeMirror(getClass.getClassLoader)\n    import scala.reflect.runtime.universe.NoSymbol\n    case class ReflectionProblem(e: Throwable, symbol: String, var count: Int)\n\n    val problems: mutable.Map[String, ReflectionProblem] = mutable.Map[String, ReflectionProblem]()\n\n    override def accept(obj: Any): Boolean = true\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject { result =>\n        val obj = scalaInfo.value\n\n        if (obj == null) {\n          return result\n        }\n        if (depth <= 0) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val startTime = System.currentTimeMillis()\n        val fields = listAccessibleProperties(scalaInfo, startTime)\n        if (fields.isEmpty) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val resolvedFields = mutable.Map[String, Any]()\n        result += (ResNames.VALUE -> resolvedFields)\n\n\n        fields.foreach { field =>\n          if (checkTimeoutError(field.name, startTime, timeout)) {\n            return result\n          }\n\n          if (field.ref != null && field.ref != field.path) {\n            resolvedFields += (field.name -> (mutable.Map[String, Any]() += (ResNames.REF -> field.ref)))\n          } else {\n            resolvedFields += (field.name -> manager.handleVariable(field, loopback, depth - 1))\n          }\n        }\n\n        result\n      }\n\n\n    override def getErrors: List[String] = problems.map(x =>\n      f\"Reflection error for ${x._2.symbol} counted ${x._2.count}.\\n\" +\n        f\"Error message: ${ExceptionUtils.getMessage(x._2.e)}\\n \" +\n        f\"Stacktrace:${ExceptionUtils.getStackTrace(x._2.e)}\").toList ++ super.getErrors\n\n    private def listAccessibleProperties(info: ScalaVariableInfo, startTime: Long): List[ScalaVariableInfo] = {\n      val instanceMirror = mirror.reflect(info.value)\n      val instanceSymbol = instanceMirror.symbol\n      val members = instanceSymbol.toType.members\n\n      val parsedMembers = mutable.MutableList[ScalaVariableInfo]()\n      members.foreach { symbol =>\n        if (checkTimeoutError(info.path, startTime, timeout))\n          return parsedMembers.toList\n        val variableInfo = get(instanceMirror, symbol, info.path)\n        if (variableInfo.isAccessible)\n          parsedMembers += variableInfo\n      }\n\n      parsedMembers.toList\n    }\n\n    private def get(instanceMirror: ru.InstanceMirror, symbol: ru.Symbol, path: String): ScalaVariableInfo = {\n      if (!problems.contains(path))\n        try {\n          // is public property\n          if (!symbol.isMethod && symbol.isTerm && (symbol.asTerm.isVar || symbol.asTerm.isVal)\n          && symbol.asTerm.getter != NoSymbol\n          && symbol.asTerm.getter.isPublic) {\n            val term = symbol.asTerm\n            val f = instanceMirror.reflectField(term)\n            val fieldPath = s\"$path.${term.name.toString.trim}\"\n            val value = f.get\n            val tpe = term.typeSignature.toString\n            return ScalaVariableInfo(isAccessible = tpe != \"<notype>\", isLazy = term.isLazy, value, tpe,\n              fieldPath, referenceManager.getRef(value, fieldPath))\n          }\n        } catch {\n          case e: Throwable => problems(path) = ReflectionProblem(e, symbol.toString, 1)\n        }\n      else\n        problems(path).count += 1\n\n      INACCESSIBLE\n    }\n  }\n\n  class PrimitiveHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean =\n      obj match {\n        case _: Byte => true\n        case _: Short => true\n        case _: Boolean => true\n        case _: Char => true\n        case _: Int => true\n        case _: Long => true\n        case _: Float => true\n        case _: Double => true\n        case _ => false\n      }\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any](\n        ResNames.VALUE -> scalaInfo.value,\n        ResNames.IS_PRIMITIVE -> 1\n      )\n  }\n\n  class SeqHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Seq[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Seq[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Seq[_]].size\n  }\n\n  class SetHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Set[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Set[_]].size\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Set[_]]\n  }\n\n  class SpecialsHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.getClass.getCanonicalName != null && obj.getClass.getCanonicalName.startsWith(\"scala.\")\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        var s = scalaInfo.value.toString\n        if (s.length>limit)\n          s = s.take(limit) + \"...\"\n        json.put(ResNames.VALUE, s)\n    }\n  }\n\n  class StringHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[String]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      var s = scalaInfo.value.asInstanceOf[String]\n      if (s.length>limit)\n        s = s.take(limit) + \"...\"\n      mutable.Map(\n        ResNames.VALUE -> s\n      )\n    }\n  }\n\n  class ThrowableHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Throwable]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val throwable = obj.asInstanceOf[Throwable]\n      val writer = new StringWriter()\n      val out = new PrintWriter(writer)\n      throwable.printStackTrace(out)\n\n      mutable.Map(\n        ResNames.VALUE -> writer.toString\n      )\n    }\n  }\n\n  class HandlerManager(enableProfiling: Boolean,\n                       timeout: Int,\n                       stringSizeLimit: Int,\n                       collectionSizeLimit: Int,\n                       referenceManager: ReferenceManager) {\n    private val handlerChain = ListBuffer[AbstractTypeHandler](\n      new NullHandler(),\n      new StringHandler(stringSizeLimit),\n      new ArrayHandler(collectionSizeLimit, timeout),\n      new JavaCollectionHandler(collectionSizeLimit, timeout),\n      new SeqHandler(collectionSizeLimit, timeout),\n      new SetHandler(collectionSizeLimit, timeout),\n      new MapHandler(collectionSizeLimit, timeout),\n      new ThrowableHandler(),\n      new SpecialsHandler(stringSizeLimit),\n      new PrimitiveHandler(),\n      new DatasetHandler(),\n      new RDDHandler(),\n      new SparkContextHandler(),\n      new SparkSessionHandler(),\n      new ObjectHandler(stringSizeLimit, this, referenceManager, timeout)\n    ).map(new HandlerWrapper(_, enableProfiling))\n\n    def getErrors: mutable.Seq[String] = handlerChain.flatMap(x => x.handler.getErrors)\n\n    def handleVariable(info: ScalaVariableInfo, loopback: Loopback, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      handlerChain.find(_.accept(info)).map(_.handle(info, loopback, depth, startTime)).getOrElse(mutable.Map[String, Any]())\n    }\n  }\n\n  class HandlerWrapper(val handler: TypeHandler, profile: Boolean) {\n    def accept(info: ScalaVariableInfo): Boolean = info.isLazy || handler.accept(info.value)\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int, initStartTime: Long): Any = {\n      val startTime = if (initStartTime != null)\n        initStartTime\n      else\n        System.currentTimeMillis()\n\n      val data = if (scalaInfo.isLazy) {\n        mutable.Map[String, Any](ResNames.LAZY -> true)\n      }\n      else {\n        try {\n          val data = handler.handle(scalaInfo, loopback, depth: Int)\n          if (data.keys.count(_ == ResNames.IS_PRIMITIVE) > 0) {\n            return data(ResNames.VALUE)\n          }\n          data\n        } catch {\n          case t: Throwable =>\n            return ExceptionUtils.getRootCauseMessage(t)\n        }\n\n      }\n      try {\n        data.put(ResNames.TYPE, calculateType(scalaInfo))\n      } catch {\n        case t: Throwable =>\n          data.put(ResNames.TYPE, ExceptionUtils.getRootCauseMessage(t))\n      }\n\n      if (profile)\n        data.put(ResNames.TIME, System.currentTimeMillis() - startTime)\n\n      data\n    }\n\n    private def calculateType(scalaInfo: ScalaVariableInfo): String = {\n      if (scalaInfo.tpe != null)\n        return scalaInfo.tpe\n\n      if (scalaInfo.value != null)\n        scalaInfo.value.getClass.getCanonicalName\n      else\n        null\n    }\n  }\n  class InterpreterHandler(val interpreter: IMain) {\n    val wrapper = new ZtoolsInterpreterWrapper(interpreter)\n\n    def getVariableNames: immutable.Seq[String] =\n      interpreter.definedSymbolList.filter { x => x.isGetter }.map(_.name.toString).distinct\n\n    def getInfo(name: String, tpe: String): ScalaVariableInfo = {\n      val obj = valueOfTerm(name).orNull\n      ScalaVariableInfo(isAccessible = true, isLazy = false, obj, tpe, name, null)\n    }\n\n    def valueOfTerm(id: String): Option[Any] = wrapper.valueOfTerm(id)\n  }\n\n  case class ScalaVariableInfo(isAccessible: Boolean,\n                               isLazy: Boolean,\n                               value: Any,\n                               tpe: String,\n                               path: String,\n                               ref: String) {\n    val name: String = if (path != null)\n      path.substring(path.lastIndexOf('.') + 1)\n    else\n      null\n  }\n\n\n\n  //noinspection TypeAnnotation\n  class ZtoolsInterpreterWrapper(val iMain: IMain) {\n\n    import scala.language.implicitConversions\n    import scala.reflect.runtime.{universe => ru}\n    import iMain.global._\n\n    import scala.util.{Try => Trying}\n\n    private lazy val importToGlobal = iMain.global mkImporter ru\n    private lazy val importToRuntime = ru.internal createImporter iMain.global\n\n    private implicit def importFromRu(sym: ru.Symbol) = importToGlobal importSymbol sym\n\n    private implicit def importToRu(sym: Symbol): ru.Symbol = importToRuntime importSymbol sym\n\n    // see https://github.com/scala/scala/pull/5852/commits/a9424205121f450dea2fe2aa281dd400a579a2b7\n    def valueOfTerm(id: String): Option[Any] = exitingTyper {\n      def fixClassBasedFullName(fullName: List[String]): List[String] = {\n        if (settings.Yreplclassbased.value) {\n          val line :: read :: rest = fullName\n          line :: read :: \"INSTANCE\" :: rest\n        } else fullName\n      }\n\n      def value(fullName: String) = {\n        val universe = iMain.runtimeMirror.universe\n        import universe.{InstanceMirror, Symbol, TermName}\n        val pkg :: rest = fixClassBasedFullName((fullName split '.').toList)\n        val top = iMain.runtimeMirror.staticPackage(pkg)\n\n        @annotation.tailrec\n        def loop(inst: InstanceMirror, cur: Symbol, path: List[String]): Option[Any] = {\n          def mirrored =\n            if (inst != null) inst\n            else iMain.runtimeMirror reflect (iMain.runtimeMirror reflectModule cur.asModule).instance\n\n          path match {\n            case last :: Nil =>\n              cur.typeSignature.decls find (x => x.name.toString == last && x.isAccessor) map { m =>\n                (mirrored reflectMethod m.asMethod).apply()\n              }\n            case next :: rest =>\n              val s = cur.typeSignature.member(TermName(next))\n              val i =\n                if (s.isModule) {\n                  if (inst == null) null\n                  else iMain.runtimeMirror reflect (inst reflectModule s.asModule).instance\n                }\n                else if (s.isAccessor) {\n                  iMain.runtimeMirror reflect (mirrored reflectMethod s.asMethod).apply()\n                }\n                else {\n                  assert(false, s.fullName)\n                  inst\n                }\n              loop(i, s, rest)\n            case Nil => None\n          }\n        }\n\n        loop(null, top, rest)\n      }\n\n      Option(iMain.symbolOfTerm(id)) filter (_.exists) flatMap (s => Trying(value(s.fullName)).toOption.flatten)\n    }\n  }\n\n  class ReferenceManager {\n    private val refMap = mutable.Map[ReferenceWrapper, String]()\n    private val refInvMap = new TrieMap[ReferenceWrapper]()\n\n    /**\n     * Returns a reference (e.g. valid path) to the object or creates a record in reference maps (and returns null).\n     *\n     * @param obj  an object we want to find a reference for (can be null)\n     * @param path path of the object e.g. myVar.myField.b\n     * @return reference path to the object obj. The method returns null if obj is null itself or\n     *         obj hasn't been mentioned earlier or in the case of AnyVal object.\n     */\n    def getRef(obj: Any, path: String): String = obj match {\n      case null | _: Unit =>\n        clearRefIfPathExists(path)\n        null\n      case ref: AnyRef =>\n        val wrapper = new ReferenceWrapper(ref)\n        if (refMap.contains(wrapper)) {\n          if (refInvMap.get(path).orNull != wrapper) clearRefIfPathExists(path)\n          refMap(wrapper)\n        } else {\n          clearRefIfPathExists(path)\n          refMap(wrapper) = path\n          refInvMap.put(path, wrapper)\n          null\n        }\n      case _ => null\n    }\n\n\n    private def clearRefIfPathExists(path: String): Unit = {\n      if (refInvMap.contains(path)) {\n        val tree = refInvMap.subtree(path)\n        tree.forEach(refMap.remove(_: ReferenceWrapper))\n      }\n    }\n  }\n\n  class ReferenceWrapper(val ref: AnyRef) {\n    override def hashCode(): Int = ref.hashCode()\n\n    override def equals(obj: Any): Boolean = obj match {\n      case value: ReferenceWrapper =>\n        ref.eq(value.ref)\n      case _ => false\n    }\n  }\n\n\n  class VariablesView(val intp: IMain,\n                      val timeout: Int,\n                      val variableTimeout: Int,\n                      val collectionSizeLimit: Int,\n                      val stringSizeLimit: Int,\n                      val blackList: List[String],\n                      val whiteList: List[String] = null,\n                      val filterUnitResults: Boolean,\n                      val enableProfiling: Boolean,\n                      val depth: Int,\n                      val interpreterResCountLimit: Int = 5) {\n    val errors: mutable.MutableList[String] = mutable.MutableList[String]()\n    private val interpreterHandler = new InterpreterHandler(intp)\n    private val referenceManager = new ReferenceManager()\n\n    private val touched = mutable.Map[String, ScalaVariableInfo]()\n\n    private val handlerManager = new HandlerManager(\n      collectionSizeLimit = collectionSizeLimit,\n      stringSizeLimit = stringSizeLimit,\n      timeout = variableTimeout,\n      referenceManager = referenceManager,\n      enableProfiling = enableProfiling\n    )\n\n    //noinspection ScalaUnusedSymbol\n    def getZtoolsJsonResult: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(\n        Map(\n          \"variables\" -> resolveVariables,\n          \"errors\" -> (errors ++ handlerManager.getErrors)\n        )\n      )\n    }\n\n    def toJson: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(resolveVariables)\n    }\n\n    def resolveVariables: mutable.Map[String, Any] = {\n      val result: mutable.Map[String, Any] = mutable.Map[String, Any]()\n      val startTime = System.currentTimeMillis()\n\n      val interpreterVariablesNames = interpreterHandler.getVariableNames\n      val finalNames = filterVariableNames(interpreterVariablesNames)\n\n      finalNames.foreach { name =>\n        val varType = interpreterHandler.interpreter.typeOfTerm(name).toString().stripPrefix(\"()\")\n        val variable = mutable.Map[String, Any]()\n\n        result += name -> variable\n        variable += ResNames.TYPE -> varType\n        if (!isUnitOrNullResult(result, name))\n          variable += ResNames.VALUE -> \"<Not calculated>\"\n      }\n\n      var passedVariablesCount = 0\n      val totalVariablesCount = finalNames.size\n\n      if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n        return result\n\n      finalNames.foreach { name =>\n        if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n          return result\n        passedVariablesCount += 1\n\n        if (!isUnitOrNullResult(result, name)) {\n\n          calculateVariable(result, name)\n        }\n      }\n      result\n    }\n\n    private def calculateVariable(result: mutable.Map[String, Any], name: String) = {\n      val valMap = result(name).asInstanceOf[mutable.Map[String, Any]]\n      try {\n        val startTime = System.currentTimeMillis()\n\n        val info = interpreterHandler.getInfo(name, valMap(ResNames.TYPE).asInstanceOf[String])\n        val ref = referenceManager.getRef(info.value, name)\n        touched(info.path) = info\n\n        if (ref != null && ref != info.path) {\n          result += (info.path -> mutable.Map[String, Any](ResNames.REF -> ref))\n        } else {\n          result += info.path -> parseInfo(info, depth, startTime)\n        }\n      } catch {\n        case t: Throwable =>\n          valMap += ResNames.VALUE -> ExceptionUtils.getRootCauseMessage(t)\n      }\n    }\n\n    private def isUnitOrNullResult(result: mutable.Map[String, Any], name: String) = {\n      val res = result(name).asInstanceOf[mutable.Map[String, Any]]\n      val valType = res(ResNames.TYPE)\n      valType == \"Unit\" || valType == \"Null\"\n    }\n\n    def resolveVariable(path: String): mutable.Map[String, Any] = {\n      val result = mutable.Map[String, Any]()\n      val obj = touched.get(path).orNull\n      if (obj.ref != null) {\n        result += (ResNames.VALUE -> mutable.Map[String, Any](ResNames.REF -> obj.ref))\n      } else {\n        result += (ResNames.VALUE -> parseInfo(obj, depth))\n      }\n      result\n    }\n\n    private def parseInfo(info: ScalaVariableInfo, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      val loopback = new Loopback {\n        override def pass(obj: Any, id: String): Any = {\n          val si = ScalaVariableInfo(isAccessible = true, isLazy = false, obj, null, id, referenceManager.getRef(obj, id))\n          parseInfo(si, depth - 1)\n        }\n      }\n      handlerManager.handleVariable(info, loopback, depth, startTime)\n    }\n\n    private def filterVariableNames(interpreterVariablesNames: Seq[String]) = {\n      val variablesNames = interpreterVariablesNames.seq\n        .filter { name => !blackList.contains(name) }\n        .filter { name => whiteList == null || whiteList.contains(name) }\n\n\n      val p = Pattern.compile(\"res\\\\d*\")\n      val (resVariables, otherVariables: immutable.Seq[String]) = variablesNames.partition(x => p.matcher(x).matches())\n      val sortedResVariables = resVariables\n        .map(res => Try(res.stripPrefix(\"res\").toInt))\n        .filter(_.isSuccess)\n        .map(_.get)\n        .sortWith(_ > _)\n        .take(interpreterResCountLimit)\n        .map(num => \"res\" + num)\n\n      val finalNames = otherVariables ++ sortedResVariables\n      finalNames\n    }\n\n    //noinspection ScalaUnusedSymbol\n    private implicit def toJavaFunction[A, B](f: A => B): JFunction[A, B] = new JFunction[A, B] {\n      override def apply(a: A): B = f(a)\n    }\n\n    private def checkTimeout(startTimeout: Long, passed: Int, total: Int): Boolean = {\n      val isTimeoutExceed = System.currentTimeMillis() - startTimeout > timeout\n      if (isTimeoutExceed)\n        errors += s\"Variables collect timeout. Exceed ${timeout}ms. Parsed $passed from $total.\"\n      isTimeoutExceed\n    }\n  }\n\n  class DatasetHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Dataset[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val df = obj.asInstanceOf[Dataset[_]]\n\n\n      val schema = df.schema\n      val jsonSchemaColumns = schema.fields.map(field => {\n        val value = withJsonObject { jsonField =>\n          jsonField += \"name\" -> wrap(field.name, null)\n          jsonField += \"nullable\" -> wrap(field.nullable, null)\n          jsonField += \"dataType\" -> wrap(field.dataType.typeName, null)\n        }\n        wrap(value, \"org.apache.spark.sql.types.StructField\")\n      }\n      )\n\n      val jsonSchema = mutable.Map(\n        ResNames.VALUE -> jsonSchemaColumns,\n        ResNames.TYPE -> \"org.apache.spark.sql.types.StructType\",\n        ResNames.LENGTH -> jsonSchemaColumns.length\n      )\n\n      val dfValue = mutable.Map(\n        \"schema()\" -> jsonSchema,\n        \"getStorageLevel()\" -> wrap(df.storageLevel.toString(), \"org.apache.spark.storage.StorageLevel\")\n      )\n\n      mutable.Map(\n        ResNames.VALUE -> dfValue\n      )\n    }\n  }\n\n\n  class RDDHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[RDD[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val rdd = obj.asInstanceOf[RDD[_]]\n        json += (ResNames.VALUE -> withJsonObject { value =>\n          value += (\"getNumPartitions()\" -> wrap(rdd.getNumPartitions, \"Int\"))\n          value += (\"name\" -> wrap(rdd.name, \"String\"))\n          value += (\"id\" -> wrap(rdd.id, \"Int\"))\n          value += (\"partitioner\" -> wrap(rdd.partitioner.toString, \"Option[org.apache.spark.Partitioner]\"))\n          value += (\"getStorageLevel()\" -> wrap(rdd.getStorageLevel.toString, \"org.apache.spark.storage.StorageLevel\"))\n        })\n    }\n  }\n\n  class SparkContextHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkContext]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val sc = scalaInfo.value.asInstanceOf[SparkContext]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"sparkUser\" -> wrap(sc.sparkUser, \"String\"))\n          json += (\"sparkTime\" -> wrap(sc.startTime, \"Long\"))\n          json += (\"applicationId()\" -> wrap(sc.applicationId, \"String\"))\n          json += (\"applicationAttemptId()\" -> wrap(sc.applicationAttemptId.toString, \"Option[String]\"))\n          json += (\"appName()\" -> sc.appName)\n        })\n    }\n  }\n\n  class SparkSessionHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkSession]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val id = scalaInfo.path\n\n        val spark = obj.asInstanceOf[SparkSession]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"version()\" -> spark.version)\n          json += (\"sparkContext\" -> loopback.pass(spark.sparkContext, s\"$id.sparkContext\"))\n        })\n    }\n  }\n\n\n  /**\n   * Main section\n   */\n  val iMain: IMain = $intp\n  val depth: Int = 2\n  val filterUnitResults: Boolean = true\n  val enableProfiling: Boolean = true\n  val collectionSizeLimit = 100\n  val stringSizeLimit = 400\n  val timeout = 5000\n  val variableTimeout = 2000\n  val interpreterResCountLimit = 10\n  val blackList = \"$intp,sqlContext,z,engine\".split(',').toList\n  val whiteList: List[String] =  null\n\n\n  val variableView = new VariablesView(\n    intp = iMain,\n    timeout = timeout,\n    variableTimeout = variableTimeout,\n    collectionSizeLimit = collectionSizeLimit,\n    stringSizeLimit = stringSizeLimit,\n    blackList = blackList,\n    whiteList = whiteList,\n    filterUnitResults = filterUnitResults,\n    enableProfiling = enableProfiling,\n    depth = depth,\n    interpreterResCountLimit = interpreterResCountLimit\n  )\n\n  implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n  val variablesJson = variableView.getZtoolsJsonResult\n  println(\"---ztools-scala---\")\n  println(variablesJson)\n  println(\"---ztools-scala---\")\n}\ncatch {\n  case t: Throwable =>\n    import org.apache.commons.lang.exception.ExceptionUtils\n    import org.json4s.jackson.Serialization\n    import org.json4s.{Formats, NoTypeHints}\n\n    implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n    val result = Serialization.write(Map(\n      \"errors\" -> Array(f\"${ExceptionUtils.getMessage(t)}\\n${ExceptionUtils.getStackTrace(t)}\")\n    ))\n    println(\"---ztools-scala---\")\n    println(result)\n    println(\"---ztools-scala---\")\n}\n{\n    var sqlTableShows: Array[String] = Array(\"SHOW TABLES  \")\n    val additionalTables = Array[Tuple2[String, String]]((\"\", \"hotel\"), (\"\", \"tipologias\"), (\"\", \"quartos_reservados\"))\n    val timeout = 5000\n    val collectOnlyTempTables = false\n    val appendOutput = false\n\n    case class ZtoolsColumn(name: String,\n                            columnType: String,\n                            description: String)\n\n    case class ZtoolsTable(name: String,\n                           databaseName: String,\n                           var columns: Array[ZtoolsColumn],\n                           var error: String = null)\n\n    case class ZtoolsSqlProfile(request: String, time: Long)\n\n    case class ZtoolsSqlInfo(tables: Array[ZtoolsTable],\n                             errors: Array[String],\n                             profiling: Array[ZtoolsSqlProfile],\n                             appendOutput: Boolean = appendOutput)\n\n\n    //TO KNOW:\n    //We collect info by spark.sql not spark.catalog because there some errors with Glue, database does not read\n    //Additionally we cannot use column name because it can be different \"namespace\" in EMR and \"database\" in vanilla spark\n    def calcZtoolsSqlSchemas(): String = {\n        import com.fasterxml.jackson.annotation.JsonAutoDetect.Visibility\n        import com.fasterxml.jackson.annotation.PropertyAccessor\n        import com.fasterxml.jackson.databind.ObjectMapper\n        import org.apache.commons.lang.exception.ExceptionUtils\n        import org.apache.spark.sql.Row\n\n        import scala.collection.mutable.ArrayBuffer\n\n        val startTime = System.currentTimeMillis()\n        val errors = ArrayBuffer[String]()\n\n        def convertThrowable(msg: String, t: Throwable): String = msg + \"\\n\" +\n                ExceptionUtils.getRootCauseMessage(t) + \"\\n\" +\n                ExceptionUtils.getStackTrace(t)\n\n        def escapeSql(string: String) = \"`\" + string.replace(\"`\", \"``\") + \"`\"\n\n\n\n\n        var tables = ArrayBuffer[ZtoolsTable]()\n        var profilingResult = ArrayBuffer[ZtoolsSqlProfile]()\n\n        def performSql(sqlRequest: String): Tuple2[Array[_ <: Row], String] = {\n            if (System.currentTimeMillis() - startTime > timeout) {\n                val error = f\"Timeout $timeout exceed. Sql request '$sqlRequest' ignored.\"\n                errors.append(error)\n                return (Array.empty, error)\n            }\n            val startTransactionTime = System.currentTimeMillis()\n            try {\n                val rows = spark.sql(sqlRequest).collect()\n                (rows, null)\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(sqlRequest, t))\n                    (Array.empty, ExceptionUtils.getMessage(t))\n            } finally {\n                profilingResult += ZtoolsSqlProfile(sqlRequest, System.currentTimeMillis() - startTransactionTime)\n            }\n        }\n\n        if (sqlTableShows!=null && sqlTableShows.isEmpty) {\n            val sqlRequest = \"show databases\"\n            val databases = performSql(sqlRequest)._1.map(_.getAs[String](0))\n            sqlTableShows = databases.map(db => f\"SHOW TABLES in $db\")\n        }\n\n        if (sqlTableShows==null) {\n            sqlTableShows = Array.empty\n        }\n\n        sqlTableShows.foreach(sqlRequest => {\n            try {\n                var listTables = performSql(sqlRequest)._1\n                if (collectOnlyTempTables)\n                    listTables = listTables.filter(_.getAs[Boolean](2) == true)\n\n                listTables.map(row => ZtoolsTable(\n                    databaseName = row.getAs[String](0),\n                    name = row.getAs[String](1),\n                    columns = Array.empty[ZtoolsColumn])).foreach(t => tables.append(t))\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(s\"Error transform output of  $sqlRequest\", t))\n                    ArrayBuffer.empty[ZtoolsTable]\n            }\n        })\n\n        val tableSet = (additionalTables.map(it => ZtoolsTable(it._2, it._1, Array.empty)) ++ tables).distinct\n\n        def processTable(table: ZtoolsTable): Unit = {\n            val columns = try {\n                val tableSqlName = if (table.databaseName == null || table.databaseName.isEmpty)\n                    escapeSql(table.name)\n                else\n                    escapeSql(table.databaseName) + \".\" + escapeSql(table.name)\n\n                //https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-describe-table.html\n                val sqlResult = performSql(s\"DESCRIBE TABLE $tableSqlName\")\n\n                val columnRows = sqlResult._1\n                table.error = sqlResult._2\n\n                //Ignore partition section\n                columnRows.takeWhile(row => !Option(row.getAs[String](0)).getOrElse(\"\").startsWith(\"# \"))\n                        .map(row => ZtoolsColumn(row.getAs[String](0), row.getAs[String](1), row.getAs[String](2)))\n            } catch {\n                case t: Throwable => convertThrowable(s\"Error list columns for ${table.name}\", t)\n                    table.error = ExceptionUtils.getRootCauseMessage(t)\n                    errors.append(convertThrowable(s\"Error list columns for ${table.name}\", t))\n                    return\n            }\n            table.columns = columns\n        }\n\n        tableSet.foreach(table => {\n            processTable(table)\n        })\n\n        val res = ZtoolsSqlInfo(tableSet.toArray, errors.toArray, profilingResult.toArray)\n        val objectMapper = new ObjectMapper().setVisibility(PropertyAccessor.FIELD, Visibility.ANY).writerWithDefaultPrettyPrinter()\n        objectMapper.writeValueAsString(res)\n    }\n\n    def ztoolsPrintResult(): Unit = {\n        val ztoolsSqlResult = calcZtoolsSqlSchemas()\n        println(\"---ztools-sql---\")\n        println(ztoolsSqlResult)\n        println(\"---ztools-sql---\")\n    }\n\n    ztoolsPrintResult()\n}",
   "config": {
    "editorHide": true,
    "tableHide": true
   },
   "dateStarted": "2023-04-29 10:51:11.314",
   "dateUpdated": "2023-04-29 10:51:17.837",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "<console>:371: \u001b[33mwarning: \u001b[0ma pure expression does nothing in statement position; you may be omitting necessary parentheses\n               result\n               ^\n<console>:546: \u001b[33mwarning: \u001b[0mcomparing values of types Long and Null using `!=' will always yield true\n             val startTime = if (initStartTime != null)\n                                               ^\n---ztools-scala---\n{\"variables\":{\"res18\":{\"type\":\"Unit\"},\"res24\":{\"type\":\"Unit\"},\"spark\":{\"type\":\"org.apache.spark.sql.SparkSession\",\"value\":{\"sparkContext\":{\"type\":\"org.apache.spark.SparkContext\",\"value\":{\"appName()\":\"48f0e575-db26-41ac-9910-3937337a502f\",\"sparkTime\":{\"type\":\"Long\",\"value\":1682758687139},\"applicationId()\":{\"type\":\"String\",\"value\":\"local-1682758688989\"},\"applicationAttemptId()\":{\"type\":\"Option[String]\",\"value\":\"None\"},\"sparkUser\":{\"type\":\"String\",\"value\":\"zeppelin\"}},\"time\":7},\"version()\":\"2.4.5\"},\"time\":47},\"csvHotel\":{\"type\":\"String\",\"value\":\"/data/tp/Hotel.csv\",\"time\":1},\"df\":{\"type\":\"org.apache.spark.sql.Dataset\",\"value\":{\"getStorageLevel()\":{\"type\":\"org.apache.spark.storage.StorageLevel\",\"value\":\"StorageLevel(disk, memory, deserialized, 1 replicas)\"},\"schema()\":{\"type\":\"org.apache.spark.sql.types.StructType\",\"length\":16,\"value\":[{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"hotel_ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Reserve ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"pais\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"estado_reserva\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Room ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Tipo de Quarto\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"rate_plan\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_reserva\"},\"dataType\":{\"type\":null,\"value\":\"timestamp\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_chegada\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_partida\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"num_noites\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"ocupacao\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"criancas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Bebés\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"preco_euros\"},\"dataType\":{\"type\":null,\"value\":\"double\"}}}]}},\"time\":7},\"res20\":{\"type\":\"Unit\"},\"res14\":{\"type\":\"Unit\"},\"lastException\":{\"type\":\"Throwable\",\"value\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`t.hotel_ID`' given input columns: [qr.data_reserva, qr.criancas, qr.Tipo de Quarto, qr.preco_euros, qr.Bebés, qr.num_noites, qr.data_chegada, t.capacidade_max_adultos, t.Capacidade máxima de camas extra (crianças), qr.data_partida, qr.Room ID, qr.hotel_ID, t.capacidade_max_bebes, t.Hotel ID, t.Capacidade máxima de camas extra, t.Capacidade mínima, qr.estado_reserva, qr.pais, qr.rate_plan, t.Capacidade máxima de berços extra, t.Capacidade mínima de crianças, t.capacidade_maxima, qr.ocupacao, t.Capacidade mínima de adultos, t.Room ID, qr.adultos, t.quantidade, qr.Reserve ID, t.tipo_quarto, t.capacidade_max_criancas]; line 1 pos 432;\\n'Project ['qr.pais, 'qr.estado_reserva, 'qr.rate_plan, 'qr.data_reserva, 'qr.data_partida, 'qr.num_noites, 'qr.ocupacao, 'qr.adultos, 'qr.criancas, 'qr.preco_euros, 'qr.data_chegada, 't.tipo_quarto, 't.quantidade, 't.capacidade_maxima, 't.capacidade_max_adultos, 't.capacidade_max_criancas, 't.capacidade_max_bebes, 'h.localizacao, 'h.estrelas, 'h.idade_max_criancas, 'h.idade_max_bebes, 'h.qtd_quartos]\\n+- 'Join Inner, ('h.hotel_ID = 'qr.hotel_ID)\\n   :- 'Join Inner, (('t.hotel_ID = hotel_ID#4410) && ('t.room_ID = 'qr.room_ID))\\n   :  :- SubqueryAlias `qr`\\n   :  :  +- SubqueryAlias `quartos_reservados`\\n   :  :     +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, adultos#4563, criancas#4580, Bebés#4392, Preço (€)#4393 AS preco_euros#4597]\\n   :  :        +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, adultos#4563, Crianças#4391 AS criancas#4580, Bebés#4392, Preço (€)#4393]\\n   :  :           +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, Adultos#4390 AS adultos#4563, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :              +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, Ocupação#4389 AS ocupacao#4546, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                 +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, Número de noites#4388 AS num_noites#4529, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                    +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, Data de partida#4387 AS data_partida#4512, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                       +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, Data chegada#4386 AS data_chegada#4495, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                          +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, Data da reserva#4385 AS data_reserva#4478, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                             +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384 AS rate_plan#4461, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, Estado da reserva#4381 AS estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                   +- Project [hotel_ID#4410, Reserve ID#4379, País#4380 AS pais#4427, Estado da reserva#4381, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                      +- Project [Hotel ID#4378 AS hotel_ID#4410, Reserve ID#4379, País#4380, Estado da reserva#4381, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                         +- Relation[Hotel ID#4378,Reserve ID#4379,País#4380,Estado da reserva#4381,Room ID#4382,Tipo de Quarto#4383,RatePlan#4384,Data da reserva#4385,Data chegada#4386,Data de partida#4387,Número de noites#4388,Ocupação#4389,Adultos#4390,Crianças#4391,Bebés#4392,Preço (€)#4393] csv\\n   :  +- SubqueryAlias `t`\\n   :     +- SubqueryAlias `tipologias`\\n   :        +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, capacidade_max_criancas#2665, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587 AS capacidade_max_bebes#2680, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :           +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585 AS capacidade_max_criancas#2665, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :              +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, Capacidade máxima de adultos#2583 AS capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                 +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, Capacidade máxima#2581 AS capacidade_maxima#2635, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                    +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, Quantidade#2580 AS quantidade#2620, Capacidade máxima#2581, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                       +- Project [Hotel ID#2577, Room ID#2578, Tipo de quarto#2579 AS tipo_quarto#2605, Quantidade#2580, Capacidade máxima#2581, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                          +- Relation[Hotel ID#2577,Room ID#2578,Tipo de quarto#2579,Quantidade#2580,Capacidade máxima#2581,Capacidade mínima#2582,Capacidade máxima de adultos#2583,Capacidade mínima de adultos#2584,Capacidade máxima de crianças#2585,Capacidade mínima de crianças#2586,Capacidade máxima de bebés#2587,Capacidade máxima de camas extra#2588,Capacidade máxima de camas extra (crianças)#2589,Capacidade máxima de berços extra#2590] csv\\n   +- SubqueryAlias `h`\\n      +- SubqueryAlias `hotel`\\n         +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, idade_max_bebes#2359, hora_max_checkin#2367, Quantidade de quartos#2319 AS qtd_quartos#2375]\\n            +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, idade_max_bebes#2359, Hora máxima de check-in#2318 AS hora_max_checkin#2367, Quantidade de quartos#2319]\\n               +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, Idade Máxima de Bebés (Meses)#2317 AS idade_max_bebes#2359, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                  +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, Idade Máxima de Crianças (Anos)#2316 AS idade_max_criancas#2351, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                     +- Project [hotel_ID#2327, localizacao#2335, Estrelas#2315 AS estrelas#2343, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                        +- Project [hotel_ID#2327, Localização#2314 AS localizacao#2335, Estrelas#2315, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                           +- Project [Hotel ID#2313 AS hotel_ID#2327, Localização#2314, Estrelas#2315, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                              +- Relation[Hotel ID#2313,Localização#2314,Estrelas#2315,Idade Máxima de Crianças (Anos)#2316,Idade Máxima de Bebés (Meses)#2317,Hora máxima de check-in#2318,Quantidade de quartos#2319] csv\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:44)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:64)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:66)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:68)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:70)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:72)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:74)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:76)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:78)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:80)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:82)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:84)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:86)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:88)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:90)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:92)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:94)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:96)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw.<init>(<console>:98)\\n\\tat $line148955214746.$read$$iw$$iw$$iw.<init>(<console>:100)\\n\\tat $line148955214746.$read$$iw$$iw.<init>(<console>:102)\\n\\tat $line148955214746.$read$$iw.<init>(<console>:104)\\n\\tat $line148955214746.$read.<init>(<console>:106)\\n\\tat $line148955214746.$read$.<init>(<console>:110)\\n\\tat $line148955214746.$read$.<clinit>(<console>)\\n\\tat $line148955214746.$eval$.$print$lzycompute(<console>:7)\\n\\tat $line148955214746.$eval$.$print(<console>:6)\\n\\tat $line148955214746.$eval.$print(<console>)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\\n\\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\\n\\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\\n\\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\\n\\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\\n\\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\\n\\tat org.apache.zeppelin.spark.SparkScala211Interpreter.scalaInterpret(SparkScala211Interpreter.scala:129)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:120)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:111)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\\n\\tat scala.Console$.withOut(Console.scala:65)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter._interpret$1(BaseSparkScalaInterpreter.scala:111)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.interpret(BaseSparkScalaInterpreter.scala:146)\\n\\tat org.apache.zeppelin.spark.SparkInterpreter.internalInterpret(SparkInterpreter.java:183)\\n\\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\\n\\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\\n\\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:852)\\n\\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:744)\\n\\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\\n\\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\\n\\tat org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\",\"time\":12},\"res17\":{\"type\":\"org.apache.spark.sql.Dataset\",\"value\":{\"getStorageLevel()\":{\"type\":\"org.apache.spark.storage.StorageLevel\",\"value\":\"StorageLevel(disk, memory, deserialized, 1 replicas)\"},\"schema()\":{\"type\":\"org.apache.spark.sql.types.StructType\",\"length\":14,\"value\":[{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Hotel ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Room ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"tipo_quarto\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"quantidade\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_maxima\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima de adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_criancas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima de crianças\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_bebes\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de camas extra\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de camas extra (crianças)\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de berços extra\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}}]}},\"time\":2},\"csvQ_Reservados\":{\"type\":\"String\",\"value\":\"/data/tp/Quartos_Reservados.csv\",\"time\":12},\"colums\":{\"type\":\"Array[String]\",\"length\":7,\"value\":[{\"type\":\"java.lang.String\",\"value\":\"hotel_ID\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"localizacao\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"estrelas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_criancas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_bebes\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"hora_max_checkin\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"qtd_quartos\",\"time\":0}],\"time\":2},\"res19\":{\"type\":\"Unit\"},\"sc\":{\"ref\":\"spark.sparkContext\"},\"csvTipologias\":{\"type\":\"String\",\"value\":\"/data/tp/Tipologias.csv\",\"time\":3},\"res13\":{\"type\":\"org.apache.spark.sql.Dataset\",\"value\":{\"getStorageLevel()\":{\"type\":\"org.apache.spark.storage.StorageLevel\",\"value\":\"StorageLevel(disk, memory, deserialized, 1 replicas)\"},\"schema()\":{\"type\":\"org.apache.spark.sql.types.StructType\",\"length\":7,\"value\":[{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"hotel_ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"localizacao\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"estrelas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"idade_max_criancas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"idade_max_bebes\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"hora_max_checkin\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"qtd_quartos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}}]}},\"time\":2},\"res22\":{\"type\":\"Unit\"},\"res16\":{\"type\":\"Unit\"},\"csvFacilities\":{\"type\":\"String\",\"value\":\"/data/tp/Facilities.csv\",\"time\":1},\"atributos\":{\"type\":\"Array[String]\",\"length\":7,\"value\":[{\"type\":\"java.lang.String\",\"value\":\"hotel_ID\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"localizacao\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"estrelas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_criancas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_bebes\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"hora_max_checkin\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"qtd_quartos\",\"time\":0}],\"time\":22},\"res21\":{\"type\":\"Unit\"}},\"errors\":[]}\n---ztools-scala---\n---ztools-sql---\n{\n  \"tables\" : [ {\n    \"name\" : \"hotel\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"hotel_ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"localizacao\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"estrelas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_bebes\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"hora_max_checkin\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"qtd_quartos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  }, {\n    \"name\" : \"tipologias\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"Hotel ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Room ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"tipo_quarto\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"quantidade\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_maxima\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima de adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima de crianças\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_bebes\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de camas extra\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de camas extra (crianças)\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de berços extra\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  }, {\n    \"name\" : \"quartos_reservados\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"hotel_ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Reserve ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"pais\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"estado_reserva\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Room ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Tipo de Quarto\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"rate_plan\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_reserva\",\n      \"columnType\" : \"timestamp\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_chegada\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_partida\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"num_noites\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"ocupacao\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Bebés\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"preco_euros\",\n      \"columnType\" : \"double\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  }, {\n    \"name\" : \"hotel\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"hotel_ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"localizacao\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"estrelas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_bebes\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"hora_max_checkin\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"qtd_quartos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  }, {\n    \"name\" : \"quartos_reservados\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"hotel_ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Reserve ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"pais\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"estado_reserva\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Room ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Tipo de Quarto\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"rate_plan\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_reserva\",\n      \"columnType\" : \"timestamp\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_chegada\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"data_partida\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"num_noites\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"ocupacao\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Bebés\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"preco_euros\",\n      \"columnType\" : \"double\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  }, {\n    \"name\" : \"tipologias\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"Hotel ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Room ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"tipo_quarto\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"quantidade\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_maxima\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima de adultos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade mínima de crianças\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"capacidade_max_bebes\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de camas extra\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de camas extra (crianças)\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"Capacidade máxima de berços extra\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  } ],\n  \"errors\" : [ ],\n  \"profiling\" : [ {\n    \"request\" : \"SHOW TABLES  \",\n    \"time\" : 18\n  }, {\n    \"request\" : \"DESCRIBE TABLE `hotel`\",\n    \"time\" : 12\n  }, {\n    \"request\" : \"DESCRIBE TABLE `tipologias`\",\n    \"time\" : 25\n  }, {\n    \"request\" : \"DESCRIBE TABLE `quartos_reservados`\",\n    \"time\" : 27\n  }, {\n    \"request\" : \"DESCRIBE TABLE `hotel`\",\n    \"time\" : 22\n  }, {\n    \"request\" : \"DESCRIBE TABLE `quartos_reservados`\",\n    \"time\" : 18\n  }, {\n    \"request\" : \"DESCRIBE TABLE `tipologias`\",\n    \"time\" : 11\n  } ],\n  \"appendOutput\" : false\n}\n---ztools-sql---\n"
     }
    ]
   },
   "dateFinished": "2023-04-29 10:51:17.837"
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "IS_INTELLIJ_SERVICE": true,
      "ZTOOLS_DEBUG_CELL_ID": "6081493f-774a-498f-a97d-d03de87b2bc8",
      "inlay": {
       "collapsed": true
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "%spark.spark\n//ZToolsId = 6081493f-774a-498f-a97d-d03de87b2bc8\n// It is generated code for integration with Big Data Tools plugin\n// Please DO NOT edit it.\ntry {\n  import org.apache.commons.lang.exception.ExceptionUtils\n  import org.apache.spark.sql.SparkSession\n\n  import java.io.{PrintWriter, StringWriter}\n  import java.util\n  import scala.collection.mutable.ListBuffer\n  import scala.collection.{immutable, mutable}\n  import scala.reflect.api.JavaUniverse\n  import scala.tools.nsc.interpreter.IMain\n  import org.json4s.jackson.Serialization\n  import org.json4s.{Formats, NoTypeHints}\n\n  import java.util.function.{Function => JFunction}\n  import java.util.regex.Pattern\n  import scala.language.implicitConversions\n  import scala.util.Try\n  import org.apache.spark.sql.Dataset\n  import org.apache.spark.rdd.RDD\n  import org.apache.spark.SparkContext\n\n  trait Loopback {\n    def pass(obj: Any, id: String): Any\n  }\n\n  object ResNames {\n    val REF = \"ref\"\n    val VALUE = \"value\"\n    val IS_PRIMITIVE = \"isPrimitive\"\n    val TYPE = \"type\"\n    val TIME = \"time\"\n    val LENGTH = \"length\"\n    val LAZY = \"lazy\"\n  }\n\n  object TrieMap {\n    class Node[T](var value: Option[T]) {\n      var children: mutable.Map[String, TrieMap.Node[T]] = _\n\n      def put(key: String, node: TrieMap.Node[T]): Option[Node[T]] = {\n        if (children == null)\n          children = mutable.Map[String, TrieMap.Node[T]]()\n        children.put(key, node)\n      }\n\n      def del(key: String): Option[Node[T]] = children.remove(key)\n\n      def forEach(func: Function[T, _]): Unit = {\n        func.apply(value.get)\n        if (children != null) children.foreach(t => t._2.forEach(func))\n      }\n    }\n\n    def split(key: String): Array[String] = {\n      var n = 0\n      var j = 0\n      for (i <- 0 until key.length) {\n        if (key.charAt(i) == '.') n += 1\n      }\n      val k = new Array[String](n + 1)\n      val sb = new mutable.StringBuilder(k.length)\n      for (i <- 0 until key.length) {\n        val ch = key.charAt(i)\n        if (ch == '.') {\n          k({\n            j += 1;\n            j - 1\n          }) = sb.toString\n          sb.setLength(0)\n        }\n        else sb.append(ch)\n      }\n      k(j) = sb.toString\n      k\n    }\n  }\n\n  class TrieMap[T] {\n    val root = new TrieMap.Node[T](null)\n\n    def subtree(key: Array[String], length: Int): TrieMap.Node[T] = {\n      var current = root\n      var i = 0\n      while ( {\n        i < length && current != null\n      }) {\n        if (current.children == null) return null\n        current = current.children.get(key(i)).orNull\n        i += 1\n      }\n      current\n    }\n\n    def put(key: Array[String], value: T): Option[TrieMap.Node[T]] = {\n      val node = subtree(key, key.length - 1)\n      node.put(key(key.length - 1), new TrieMap.Node[T](Option.apply(value)))\n    }\n\n    def put(key: String, value: T): Option[TrieMap.Node[T]] = {\n      val k = TrieMap.split(key)\n      put(k, value)\n    }\n\n    def contains(key: String): Boolean = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      node != null\n    }\n\n    def get(key: String): Option[T] = {\n      val k = TrieMap.split(key)\n      val node = subtree(k, k.length)\n      if (node == null) return Option.empty\n      node.value\n    }\n\n    def subtree(key: String): TrieMap.Node[T] = {\n      val k = TrieMap.split(key)\n      subtree(k, k.length)\n    }\n  }\n\n  trait TypeHandler {\n    def accept(obj: Any): Boolean\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any]\n\n    def getErrors: List[String] = List[String]()\n  }\n\n  abstract class AbstractCollectionHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    trait Iterator {\n      def hasNext: Boolean\n\n      def next: Any\n    }\n\n    def iterator(obj: Any): Iterator\n\n    def length(obj: Any): Int\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      if (depth <= 0) {\n        withJsonObject { result =>\n          var s = scalaInfo.value.toString\n          if (s.length>1000)\n            s = s.take(1000) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n      } else {\n        mutable.Map[String, Any](\n          ResNames.LENGTH -> length(scalaInfo.value),\n          ResNames.VALUE -> withJsonArray { json =>\n            val startTime = System.currentTimeMillis()\n            val it = iterator(scalaInfo.value)\n            var index = 0\n            while (it.hasNext && index < limit && !checkTimeoutError(scalaInfo.path, startTime, timeout)) {\n                val id = scalaInfo.path\n                json += loopback.pass(it.next, s\"$id[$index]\")\n                index += 1\n            }\n            })\n      }\n    }\n  }\n\n  abstract class AbstractTypeHandler extends TypeHandler {\n    val timeoutErrors: mutable.MutableList[String] = mutable.MutableList()\n\n    override def getErrors: List[String] = timeoutErrors.toList\n\n    protected def withJsonArray(body: mutable.MutableList[Any] => Unit): mutable.MutableList[Any] = {\n      val arr = mutable.MutableList[Any]()\n      body(arr)\n      arr\n    }\n\n    protected def withJsonObject(body: mutable.Map[String, Any] => Unit): mutable.Map[String, Any] = {\n      val obj = mutable.Map[String, Any]()\n      body(obj)\n      obj\n    }\n\n    protected def wrap(obj: Any, tpe: String): mutable.Map[String, Any] = mutable.Map[String, Any](\n      ResNames.VALUE -> Option(obj).orNull,\n      ResNames.TYPE -> tpe\n    )\n\n    protected def checkTimeoutError(name: String, startTime: Long, timeout: Int): Boolean = {\n      val isTimeout = System.currentTimeMillis() - startTime > timeout\n      if (isTimeout)\n        timeoutErrors += f\"Variable $name collect timeout exceed ${timeout}ms.\"\n      isTimeout\n    }\n\n  }\n\n  class ArrayHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Array[_]]\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Array[_]].length\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Array[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next\n    }\n  }\n\n  class JavaCollectionHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[util.Collection[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator() {\n      private val it = obj.asInstanceOf[util.Collection[_]].iterator()\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[util.Collection[_]].size()\n  }\n  class MapHandler(limit: Int, timeout: Int) extends AbstractTypeHandler {\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject {\n        json =>\n          val obj = scalaInfo.value\n          val id = scalaInfo.path\n          val map = obj.asInstanceOf[Map[_, _]]\n          val keys = mutable.MutableList[Any]()\n          val values = mutable.MutableList[Any]()\n          json += (\"jvm-type\" -> obj.getClass.getCanonicalName)\n          json += (\"length\" -> map.size)\n          var index = 0\n\n          json += (\"key\" -> keys)\n          json += (\"value\" -> values)\n\n          val startTime = System.currentTimeMillis()\n          map.view.take(math.min(limit, map.size)).foreach {\n            case (key, value) =>\n              if (checkTimeoutError(scalaInfo.path, startTime, timeout))\n                return json\n              keys += loopback.pass(key, s\"$id.key[$index]\")\n              values += loopback.pass(value, s\"$id.value[$index]\")\n              index += 1\n          }\n      }\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Map[_, _]]\n  }\n\n  class NullHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj == null\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any]()\n  }\n\n  class ObjectHandler(val stringSizeLimit: Int,\n                      val manager: HandlerManager,\n                      val referenceManager: ReferenceManager,\n                      val timeout: Int) extends AbstractTypeHandler {\n    private val INACCESSIBLE = ScalaVariableInfo(isAccessible = false, isLazy = false, null, null, null, null)\n    val ru: JavaUniverse = scala.reflect.runtime.universe\n    val mirror: ru.Mirror = ru.runtimeMirror(getClass.getClassLoader)\n    import scala.reflect.runtime.universe.NoSymbol\n    case class ReflectionProblem(e: Throwable, symbol: String, var count: Int)\n\n    val problems: mutable.Map[String, ReflectionProblem] = mutable.Map[String, ReflectionProblem]()\n\n    override def accept(obj: Any): Boolean = true\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      withJsonObject { result =>\n        val obj = scalaInfo.value\n\n        if (obj == null) {\n          return result\n        }\n        if (depth <= 0) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val startTime = System.currentTimeMillis()\n        val fields = listAccessibleProperties(scalaInfo, startTime)\n        if (fields.isEmpty) {\n          var s = obj.toString\n          if (s.length>stringSizeLimit)\n            s = s.take(stringSizeLimit) + \"...\"\n          result += (ResNames.VALUE -> s)\n          return result\n        }\n\n        val resolvedFields = mutable.Map[String, Any]()\n        result += (ResNames.VALUE -> resolvedFields)\n\n\n        fields.foreach { field =>\n          if (checkTimeoutError(field.name, startTime, timeout)) {\n            return result\n          }\n\n          if (field.ref != null && field.ref != field.path) {\n            resolvedFields += (field.name -> (mutable.Map[String, Any]() += (ResNames.REF -> field.ref)))\n          } else {\n            resolvedFields += (field.name -> manager.handleVariable(field, loopback, depth - 1))\n          }\n        }\n\n        result\n      }\n\n\n    override def getErrors: List[String] = problems.map(x =>\n      f\"Reflection error for ${x._2.symbol} counted ${x._2.count}.\\n\" +\n        f\"Error message: ${ExceptionUtils.getMessage(x._2.e)}\\n \" +\n        f\"Stacktrace:${ExceptionUtils.getStackTrace(x._2.e)}\").toList ++ super.getErrors\n\n    private def listAccessibleProperties(info: ScalaVariableInfo, startTime: Long): List[ScalaVariableInfo] = {\n      val instanceMirror = mirror.reflect(info.value)\n      val instanceSymbol = instanceMirror.symbol\n      val members = instanceSymbol.toType.members\n\n      val parsedMembers = mutable.MutableList[ScalaVariableInfo]()\n      members.foreach { symbol =>\n        if (checkTimeoutError(info.path, startTime, timeout))\n          return parsedMembers.toList\n        val variableInfo = get(instanceMirror, symbol, info.path)\n        if (variableInfo.isAccessible)\n          parsedMembers += variableInfo\n      }\n\n      parsedMembers.toList\n    }\n\n    private def get(instanceMirror: ru.InstanceMirror, symbol: ru.Symbol, path: String): ScalaVariableInfo = {\n      if (!problems.contains(path))\n        try {\n          // is public property\n          if (!symbol.isMethod && symbol.isTerm && (symbol.asTerm.isVar || symbol.asTerm.isVal)\n          && symbol.asTerm.getter != NoSymbol\n          && symbol.asTerm.getter.isPublic) {\n            val term = symbol.asTerm\n            val f = instanceMirror.reflectField(term)\n            val fieldPath = s\"$path.${term.name.toString.trim}\"\n            val value = f.get\n            val tpe = term.typeSignature.toString\n            return ScalaVariableInfo(isAccessible = tpe != \"<notype>\", isLazy = term.isLazy, value, tpe,\n              fieldPath, referenceManager.getRef(value, fieldPath))\n          }\n        } catch {\n          case e: Throwable => problems(path) = ReflectionProblem(e, symbol.toString, 1)\n        }\n      else\n        problems(path).count += 1\n\n      INACCESSIBLE\n    }\n  }\n\n  class PrimitiveHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean =\n      obj match {\n        case _: Byte => true\n        case _: Short => true\n        case _: Boolean => true\n        case _: Char => true\n        case _: Int => true\n        case _: Long => true\n        case _: Float => true\n        case _: Double => true\n        case _ => false\n      }\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] =\n      mutable.Map[String, Any](\n        ResNames.VALUE -> scalaInfo.value,\n        ResNames.IS_PRIMITIVE -> 1\n      )\n  }\n\n  class SeqHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Seq[_]]\n\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Seq[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Seq[_]].size\n  }\n\n  class SetHandler(limit: Int, timeout: Int) extends AbstractCollectionHandler(limit, timeout) {\n    override def iterator(obj: Any): Iterator = new Iterator {\n      private val it = obj.asInstanceOf[Set[_]].iterator\n\n      override def hasNext: Boolean = it.hasNext\n\n      override def next: Any = it.next()\n    }\n\n    override def length(obj: Any): Int = obj.asInstanceOf[Set[_]].size\n\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Set[_]]\n  }\n\n  class SpecialsHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.getClass.getCanonicalName != null && obj.getClass.getCanonicalName.startsWith(\"scala.\")\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        var s = scalaInfo.value.toString\n        if (s.length>limit)\n          s = s.take(limit) + \"...\"\n        json.put(ResNames.VALUE, s)\n    }\n  }\n\n  class StringHandler(limit: Int) extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[String]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      var s = scalaInfo.value.asInstanceOf[String]\n      if (s.length>limit)\n        s = s.take(limit) + \"...\"\n      mutable.Map(\n        ResNames.VALUE -> s\n      )\n    }\n  }\n\n  class ThrowableHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Throwable]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val throwable = obj.asInstanceOf[Throwable]\n      val writer = new StringWriter()\n      val out = new PrintWriter(writer)\n      throwable.printStackTrace(out)\n\n      mutable.Map(\n        ResNames.VALUE -> writer.toString\n      )\n    }\n  }\n\n  class HandlerManager(enableProfiling: Boolean,\n                       timeout: Int,\n                       stringSizeLimit: Int,\n                       collectionSizeLimit: Int,\n                       referenceManager: ReferenceManager) {\n    private val handlerChain = ListBuffer[AbstractTypeHandler](\n      new NullHandler(),\n      new StringHandler(stringSizeLimit),\n      new ArrayHandler(collectionSizeLimit, timeout),\n      new JavaCollectionHandler(collectionSizeLimit, timeout),\n      new SeqHandler(collectionSizeLimit, timeout),\n      new SetHandler(collectionSizeLimit, timeout),\n      new MapHandler(collectionSizeLimit, timeout),\n      new ThrowableHandler(),\n      new SpecialsHandler(stringSizeLimit),\n      new PrimitiveHandler(),\n      new DatasetHandler(),\n      new RDDHandler(),\n      new SparkContextHandler(),\n      new SparkSessionHandler(),\n      new ObjectHandler(stringSizeLimit, this, referenceManager, timeout)\n    ).map(new HandlerWrapper(_, enableProfiling))\n\n    def getErrors: mutable.Seq[String] = handlerChain.flatMap(x => x.handler.getErrors)\n\n    def handleVariable(info: ScalaVariableInfo, loopback: Loopback, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      handlerChain.find(_.accept(info)).map(_.handle(info, loopback, depth, startTime)).getOrElse(mutable.Map[String, Any]())\n    }\n  }\n\n  class HandlerWrapper(val handler: TypeHandler, profile: Boolean) {\n    def accept(info: ScalaVariableInfo): Boolean = info.isLazy || handler.accept(info.value)\n\n    def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int, initStartTime: Long): Any = {\n      val startTime = if (initStartTime != null)\n        initStartTime\n      else\n        System.currentTimeMillis()\n\n      val data = if (scalaInfo.isLazy) {\n        mutable.Map[String, Any](ResNames.LAZY -> true)\n      }\n      else {\n        try {\n          val data = handler.handle(scalaInfo, loopback, depth: Int)\n          if (data.keys.count(_ == ResNames.IS_PRIMITIVE) > 0) {\n            return data(ResNames.VALUE)\n          }\n          data\n        } catch {\n          case t: Throwable =>\n            return ExceptionUtils.getRootCauseMessage(t)\n        }\n\n      }\n      try {\n        data.put(ResNames.TYPE, calculateType(scalaInfo))\n      } catch {\n        case t: Throwable =>\n          data.put(ResNames.TYPE, ExceptionUtils.getRootCauseMessage(t))\n      }\n\n      if (profile)\n        data.put(ResNames.TIME, System.currentTimeMillis() - startTime)\n\n      data\n    }\n\n    private def calculateType(scalaInfo: ScalaVariableInfo): String = {\n      if (scalaInfo.tpe != null)\n        return scalaInfo.tpe\n\n      if (scalaInfo.value != null)\n        scalaInfo.value.getClass.getCanonicalName\n      else\n        null\n    }\n  }\n  class InterpreterHandler(val interpreter: IMain) {\n    val wrapper = new ZtoolsInterpreterWrapper(interpreter)\n\n    def getVariableNames: immutable.Seq[String] =\n      interpreter.definedSymbolList.filter { x => x.isGetter }.map(_.name.toString).distinct\n\n    def getInfo(name: String, tpe: String): ScalaVariableInfo = {\n      val obj = valueOfTerm(name).orNull\n      ScalaVariableInfo(isAccessible = true, isLazy = false, obj, tpe, name, null)\n    }\n\n    def valueOfTerm(id: String): Option[Any] = wrapper.valueOfTerm(id)\n  }\n\n  case class ScalaVariableInfo(isAccessible: Boolean,\n                               isLazy: Boolean,\n                               value: Any,\n                               tpe: String,\n                               path: String,\n                               ref: String) {\n    val name: String = if (path != null)\n      path.substring(path.lastIndexOf('.') + 1)\n    else\n      null\n  }\n\n\n\n  //noinspection TypeAnnotation\n  class ZtoolsInterpreterWrapper(val iMain: IMain) {\n\n    import scala.language.implicitConversions\n    import scala.reflect.runtime.{universe => ru}\n    import iMain.global._\n\n    import scala.util.{Try => Trying}\n\n    private lazy val importToGlobal = iMain.global mkImporter ru\n    private lazy val importToRuntime = ru.internal createImporter iMain.global\n\n    private implicit def importFromRu(sym: ru.Symbol) = importToGlobal importSymbol sym\n\n    private implicit def importToRu(sym: Symbol): ru.Symbol = importToRuntime importSymbol sym\n\n    // see https://github.com/scala/scala/pull/5852/commits/a9424205121f450dea2fe2aa281dd400a579a2b7\n    def valueOfTerm(id: String): Option[Any] = exitingTyper {\n      def fixClassBasedFullName(fullName: List[String]): List[String] = {\n        if (settings.Yreplclassbased.value) {\n          val line :: read :: rest = fullName\n          line :: read :: \"INSTANCE\" :: rest\n        } else fullName\n      }\n\n      def value(fullName: String) = {\n        val universe = iMain.runtimeMirror.universe\n        import universe.{InstanceMirror, Symbol, TermName}\n        val pkg :: rest = fixClassBasedFullName((fullName split '.').toList)\n        val top = iMain.runtimeMirror.staticPackage(pkg)\n\n        @annotation.tailrec\n        def loop(inst: InstanceMirror, cur: Symbol, path: List[String]): Option[Any] = {\n          def mirrored =\n            if (inst != null) inst\n            else iMain.runtimeMirror reflect (iMain.runtimeMirror reflectModule cur.asModule).instance\n\n          path match {\n            case last :: Nil =>\n              cur.typeSignature.decls find (x => x.name.toString == last && x.isAccessor) map { m =>\n                (mirrored reflectMethod m.asMethod).apply()\n              }\n            case next :: rest =>\n              val s = cur.typeSignature.member(TermName(next))\n              val i =\n                if (s.isModule) {\n                  if (inst == null) null\n                  else iMain.runtimeMirror reflect (inst reflectModule s.asModule).instance\n                }\n                else if (s.isAccessor) {\n                  iMain.runtimeMirror reflect (mirrored reflectMethod s.asMethod).apply()\n                }\n                else {\n                  assert(false, s.fullName)\n                  inst\n                }\n              loop(i, s, rest)\n            case Nil => None\n          }\n        }\n\n        loop(null, top, rest)\n      }\n\n      Option(iMain.symbolOfTerm(id)) filter (_.exists) flatMap (s => Trying(value(s.fullName)).toOption.flatten)\n    }\n  }\n\n  class ReferenceManager {\n    private val refMap = mutable.Map[ReferenceWrapper, String]()\n    private val refInvMap = new TrieMap[ReferenceWrapper]()\n\n    /**\n     * Returns a reference (e.g. valid path) to the object or creates a record in reference maps (and returns null).\n     *\n     * @param obj  an object we want to find a reference for (can be null)\n     * @param path path of the object e.g. myVar.myField.b\n     * @return reference path to the object obj. The method returns null if obj is null itself or\n     *         obj hasn't been mentioned earlier or in the case of AnyVal object.\n     */\n    def getRef(obj: Any, path: String): String = obj match {\n      case null | _: Unit =>\n        clearRefIfPathExists(path)\n        null\n      case ref: AnyRef =>\n        val wrapper = new ReferenceWrapper(ref)\n        if (refMap.contains(wrapper)) {\n          if (refInvMap.get(path).orNull != wrapper) clearRefIfPathExists(path)\n          refMap(wrapper)\n        } else {\n          clearRefIfPathExists(path)\n          refMap(wrapper) = path\n          refInvMap.put(path, wrapper)\n          null\n        }\n      case _ => null\n    }\n\n\n    private def clearRefIfPathExists(path: String): Unit = {\n      if (refInvMap.contains(path)) {\n        val tree = refInvMap.subtree(path)\n        tree.forEach(refMap.remove(_: ReferenceWrapper))\n      }\n    }\n  }\n\n  class ReferenceWrapper(val ref: AnyRef) {\n    override def hashCode(): Int = ref.hashCode()\n\n    override def equals(obj: Any): Boolean = obj match {\n      case value: ReferenceWrapper =>\n        ref.eq(value.ref)\n      case _ => false\n    }\n  }\n\n\n  class VariablesView(val intp: IMain,\n                      val timeout: Int,\n                      val variableTimeout: Int,\n                      val collectionSizeLimit: Int,\n                      val stringSizeLimit: Int,\n                      val blackList: List[String],\n                      val whiteList: List[String] = null,\n                      val filterUnitResults: Boolean,\n                      val enableProfiling: Boolean,\n                      val depth: Int,\n                      val interpreterResCountLimit: Int = 5) {\n    val errors: mutable.MutableList[String] = mutable.MutableList[String]()\n    private val interpreterHandler = new InterpreterHandler(intp)\n    private val referenceManager = new ReferenceManager()\n\n    private val touched = mutable.Map[String, ScalaVariableInfo]()\n\n    private val handlerManager = new HandlerManager(\n      collectionSizeLimit = collectionSizeLimit,\n      stringSizeLimit = stringSizeLimit,\n      timeout = variableTimeout,\n      referenceManager = referenceManager,\n      enableProfiling = enableProfiling\n    )\n\n    //noinspection ScalaUnusedSymbol\n    def getZtoolsJsonResult: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(\n        Map(\n          \"variables\" -> resolveVariables,\n          \"errors\" -> (errors ++ handlerManager.getErrors)\n        )\n      )\n    }\n\n    def toJson: String = {\n      implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n      Serialization.write(resolveVariables)\n    }\n\n    def resolveVariables: mutable.Map[String, Any] = {\n      val result: mutable.Map[String, Any] = mutable.Map[String, Any]()\n      val startTime = System.currentTimeMillis()\n\n      val interpreterVariablesNames = interpreterHandler.getVariableNames\n      val finalNames = filterVariableNames(interpreterVariablesNames)\n\n      finalNames.foreach { name =>\n        val varType = interpreterHandler.interpreter.typeOfTerm(name).toString().stripPrefix(\"()\")\n        val variable = mutable.Map[String, Any]()\n\n        result += name -> variable\n        variable += ResNames.TYPE -> varType\n        if (!isUnitOrNullResult(result, name))\n          variable += ResNames.VALUE -> \"<Not calculated>\"\n      }\n\n      var passedVariablesCount = 0\n      val totalVariablesCount = finalNames.size\n\n      if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n        return result\n\n      finalNames.foreach { name =>\n        if (checkTimeout(startTime, passedVariablesCount, totalVariablesCount))\n          return result\n        passedVariablesCount += 1\n\n        if (!isUnitOrNullResult(result, name)) {\n\n          calculateVariable(result, name)\n        }\n      }\n      result\n    }\n\n    private def calculateVariable(result: mutable.Map[String, Any], name: String) = {\n      val valMap = result(name).asInstanceOf[mutable.Map[String, Any]]\n      try {\n        val startTime = System.currentTimeMillis()\n\n        val info = interpreterHandler.getInfo(name, valMap(ResNames.TYPE).asInstanceOf[String])\n        val ref = referenceManager.getRef(info.value, name)\n        touched(info.path) = info\n\n        if (ref != null && ref != info.path) {\n          result += (info.path -> mutable.Map[String, Any](ResNames.REF -> ref))\n        } else {\n          result += info.path -> parseInfo(info, depth, startTime)\n        }\n      } catch {\n        case t: Throwable =>\n          valMap += ResNames.VALUE -> ExceptionUtils.getRootCauseMessage(t)\n      }\n    }\n\n    private def isUnitOrNullResult(result: mutable.Map[String, Any], name: String) = {\n      val res = result(name).asInstanceOf[mutable.Map[String, Any]]\n      val valType = res(ResNames.TYPE)\n      valType == \"Unit\" || valType == \"Null\"\n    }\n\n    def resolveVariable(path: String): mutable.Map[String, Any] = {\n      val result = mutable.Map[String, Any]()\n      val obj = touched.get(path).orNull\n      if (obj.ref != null) {\n        result += (ResNames.VALUE -> mutable.Map[String, Any](ResNames.REF -> obj.ref))\n      } else {\n        result += (ResNames.VALUE -> parseInfo(obj, depth))\n      }\n      result\n    }\n\n    private def parseInfo(info: ScalaVariableInfo, depth: Int, startTime: Long = System.currentTimeMillis()): Any = {\n      val loopback = new Loopback {\n        override def pass(obj: Any, id: String): Any = {\n          val si = ScalaVariableInfo(isAccessible = true, isLazy = false, obj, null, id, referenceManager.getRef(obj, id))\n          parseInfo(si, depth - 1)\n        }\n      }\n      handlerManager.handleVariable(info, loopback, depth, startTime)\n    }\n\n    private def filterVariableNames(interpreterVariablesNames: Seq[String]) = {\n      val variablesNames = interpreterVariablesNames.seq\n        .filter { name => !blackList.contains(name) }\n        .filter { name => whiteList == null || whiteList.contains(name) }\n\n\n      val p = Pattern.compile(\"res\\\\d*\")\n      val (resVariables, otherVariables: immutable.Seq[String]) = variablesNames.partition(x => p.matcher(x).matches())\n      val sortedResVariables = resVariables\n        .map(res => Try(res.stripPrefix(\"res\").toInt))\n        .filter(_.isSuccess)\n        .map(_.get)\n        .sortWith(_ > _)\n        .take(interpreterResCountLimit)\n        .map(num => \"res\" + num)\n\n      val finalNames = otherVariables ++ sortedResVariables\n      finalNames\n    }\n\n    //noinspection ScalaUnusedSymbol\n    private implicit def toJavaFunction[A, B](f: A => B): JFunction[A, B] = new JFunction[A, B] {\n      override def apply(a: A): B = f(a)\n    }\n\n    private def checkTimeout(startTimeout: Long, passed: Int, total: Int): Boolean = {\n      val isTimeoutExceed = System.currentTimeMillis() - startTimeout > timeout\n      if (isTimeoutExceed)\n        errors += s\"Variables collect timeout. Exceed ${timeout}ms. Parsed $passed from $total.\"\n      isTimeoutExceed\n    }\n  }\n\n  class DatasetHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[Dataset[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = {\n      val obj = scalaInfo.value\n      val df = obj.asInstanceOf[Dataset[_]]\n\n\n      val schema = df.schema\n      val jsonSchemaColumns = schema.fields.map(field => {\n        val value = withJsonObject { jsonField =>\n          jsonField += \"name\" -> wrap(field.name, null)\n          jsonField += \"nullable\" -> wrap(field.nullable, null)\n          jsonField += \"dataType\" -> wrap(field.dataType.typeName, null)\n        }\n        wrap(value, \"org.apache.spark.sql.types.StructField\")\n      }\n      )\n\n      val jsonSchema = mutable.Map(\n        ResNames.VALUE -> jsonSchemaColumns,\n        ResNames.TYPE -> \"org.apache.spark.sql.types.StructType\",\n        ResNames.LENGTH -> jsonSchemaColumns.length\n      )\n\n      val dfValue = mutable.Map(\n        \"schema()\" -> jsonSchema,\n        \"getStorageLevel()\" -> wrap(df.storageLevel.toString(), \"org.apache.spark.storage.StorageLevel\")\n      )\n\n      mutable.Map(\n        ResNames.VALUE -> dfValue\n      )\n    }\n  }\n\n\n  class RDDHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[RDD[_]]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val rdd = obj.asInstanceOf[RDD[_]]\n        json += (ResNames.VALUE -> withJsonObject { value =>\n          value += (\"getNumPartitions()\" -> wrap(rdd.getNumPartitions, \"Int\"))\n          value += (\"name\" -> wrap(rdd.name, \"String\"))\n          value += (\"id\" -> wrap(rdd.id, \"Int\"))\n          value += (\"partitioner\" -> wrap(rdd.partitioner.toString, \"Option[org.apache.spark.Partitioner]\"))\n          value += (\"getStorageLevel()\" -> wrap(rdd.getStorageLevel.toString, \"org.apache.spark.storage.StorageLevel\"))\n        })\n    }\n  }\n\n  class SparkContextHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkContext]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val sc = scalaInfo.value.asInstanceOf[SparkContext]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"sparkUser\" -> wrap(sc.sparkUser, \"String\"))\n          json += (\"sparkTime\" -> wrap(sc.startTime, \"Long\"))\n          json += (\"applicationId()\" -> wrap(sc.applicationId, \"String\"))\n          json += (\"applicationAttemptId()\" -> wrap(sc.applicationAttemptId.toString, \"Option[String]\"))\n          json += (\"appName()\" -> sc.appName)\n        })\n    }\n  }\n\n  class SparkSessionHandler extends AbstractTypeHandler {\n    override def accept(obj: Any): Boolean = obj.isInstanceOf[SparkSession]\n\n    override def handle(scalaInfo: ScalaVariableInfo, loopback: Loopback, depth: Int): mutable.Map[String, Any] = withJsonObject {\n      json =>\n        val obj = scalaInfo.value\n        val id = scalaInfo.path\n\n        val spark = obj.asInstanceOf[SparkSession]\n        json += (ResNames.VALUE -> withJsonObject { json =>\n          json += (\"version()\" -> spark.version)\n          json += (\"sparkContext\" -> loopback.pass(spark.sparkContext, s\"$id.sparkContext\"))\n        })\n    }\n  }\n\n\n  /**\n   * Main section\n   */\n  val iMain: IMain = $intp\n  val depth: Int = 2\n  val filterUnitResults: Boolean = true\n  val enableProfiling: Boolean = true\n  val collectionSizeLimit = 100\n  val stringSizeLimit = 400\n  val timeout = 5000\n  val variableTimeout = 2000\n  val interpreterResCountLimit = 10\n  val blackList = \"$intp,sqlContext,z,engine\".split(',').toList\n  val whiteList: List[String] =  null\n\n\n  val variableView = new VariablesView(\n    intp = iMain,\n    timeout = timeout,\n    variableTimeout = variableTimeout,\n    collectionSizeLimit = collectionSizeLimit,\n    stringSizeLimit = stringSizeLimit,\n    blackList = blackList,\n    whiteList = whiteList,\n    filterUnitResults = filterUnitResults,\n    enableProfiling = enableProfiling,\n    depth = depth,\n    interpreterResCountLimit = interpreterResCountLimit\n  )\n\n  implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n  val variablesJson = variableView.getZtoolsJsonResult\n  println(\"---ztools-scala---\")\n  println(variablesJson)\n  println(\"---ztools-scala---\")\n}\ncatch {\n  case t: Throwable =>\n    import org.apache.commons.lang.exception.ExceptionUtils\n    import org.json4s.jackson.Serialization\n    import org.json4s.{Formats, NoTypeHints}\n\n    implicit val ztoolsFormats: AnyRef with Formats = Serialization.formats(NoTypeHints)\n    val result = Serialization.write(Map(\n      \"errors\" -> Array(f\"${ExceptionUtils.getMessage(t)}\\n${ExceptionUtils.getStackTrace(t)}\")\n    ))\n    println(\"---ztools-scala---\")\n    println(result)\n    println(\"---ztools-scala---\")\n}\n{\n    var sqlTableShows: Array[String] = null\n    val additionalTables = Array[Tuple2[String, String]]((\"\", \"hotel\"))\n    val timeout = 5000\n    val collectOnlyTempTables = false\n    val appendOutput = true\n\n    case class ZtoolsColumn(name: String,\n                            columnType: String,\n                            description: String)\n\n    case class ZtoolsTable(name: String,\n                           databaseName: String,\n                           var columns: Array[ZtoolsColumn],\n                           var error: String = null)\n\n    case class ZtoolsSqlProfile(request: String, time: Long)\n\n    case class ZtoolsSqlInfo(tables: Array[ZtoolsTable],\n                             errors: Array[String],\n                             profiling: Array[ZtoolsSqlProfile],\n                             appendOutput: Boolean = appendOutput)\n\n\n    //TO KNOW:\n    //We collect info by spark.sql not spark.catalog because there some errors with Glue, database does not read\n    //Additionally we cannot use column name because it can be different \"namespace\" in EMR and \"database\" in vanilla spark\n    def calcZtoolsSqlSchemas(): String = {\n        import com.fasterxml.jackson.annotation.JsonAutoDetect.Visibility\n        import com.fasterxml.jackson.annotation.PropertyAccessor\n        import com.fasterxml.jackson.databind.ObjectMapper\n        import org.apache.commons.lang.exception.ExceptionUtils\n        import org.apache.spark.sql.Row\n\n        import scala.collection.mutable.ArrayBuffer\n\n        val startTime = System.currentTimeMillis()\n        val errors = ArrayBuffer[String]()\n\n        def convertThrowable(msg: String, t: Throwable): String = msg + \"\\n\" +\n                ExceptionUtils.getRootCauseMessage(t) + \"\\n\" +\n                ExceptionUtils.getStackTrace(t)\n\n        def escapeSql(string: String) = \"`\" + string.replace(\"`\", \"``\") + \"`\"\n\n\n\n\n        var tables = ArrayBuffer[ZtoolsTable]()\n        var profilingResult = ArrayBuffer[ZtoolsSqlProfile]()\n\n        def performSql(sqlRequest: String): Tuple2[Array[_ <: Row], String] = {\n            if (System.currentTimeMillis() - startTime > timeout) {\n                val error = f\"Timeout $timeout exceed. Sql request '$sqlRequest' ignored.\"\n                errors.append(error)\n                return (Array.empty, error)\n            }\n            val startTransactionTime = System.currentTimeMillis()\n            try {\n                val rows = spark.sql(sqlRequest).collect()\n                (rows, null)\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(sqlRequest, t))\n                    (Array.empty, ExceptionUtils.getMessage(t))\n            } finally {\n                profilingResult += ZtoolsSqlProfile(sqlRequest, System.currentTimeMillis() - startTransactionTime)\n            }\n        }\n\n        if (sqlTableShows!=null && sqlTableShows.isEmpty) {\n            val sqlRequest = \"show databases\"\n            val databases = performSql(sqlRequest)._1.map(_.getAs[String](0))\n            sqlTableShows = databases.map(db => f\"SHOW TABLES in $db\")\n        }\n\n        if (sqlTableShows==null) {\n            sqlTableShows = Array.empty\n        }\n\n        sqlTableShows.foreach(sqlRequest => {\n            try {\n                var listTables = performSql(sqlRequest)._1\n                if (collectOnlyTempTables)\n                    listTables = listTables.filter(_.getAs[Boolean](2) == true)\n\n                listTables.map(row => ZtoolsTable(\n                    databaseName = row.getAs[String](0),\n                    name = row.getAs[String](1),\n                    columns = Array.empty[ZtoolsColumn])).foreach(t => tables.append(t))\n            } catch {\n                case t: Throwable =>\n                    errors.append(convertThrowable(s\"Error transform output of  $sqlRequest\", t))\n                    ArrayBuffer.empty[ZtoolsTable]\n            }\n        })\n\n        val tableSet = (additionalTables.map(it => ZtoolsTable(it._2, it._1, Array.empty)) ++ tables).distinct\n\n        def processTable(table: ZtoolsTable): Unit = {\n            val columns = try {\n                val tableSqlName = if (table.databaseName == null || table.databaseName.isEmpty)\n                    escapeSql(table.name)\n                else\n                    escapeSql(table.databaseName) + \".\" + escapeSql(table.name)\n\n                //https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-describe-table.html\n                val sqlResult = performSql(s\"DESCRIBE TABLE $tableSqlName\")\n\n                val columnRows = sqlResult._1\n                table.error = sqlResult._2\n\n                //Ignore partition section\n                columnRows.takeWhile(row => !Option(row.getAs[String](0)).getOrElse(\"\").startsWith(\"# \"))\n                        .map(row => ZtoolsColumn(row.getAs[String](0), row.getAs[String](1), row.getAs[String](2)))\n            } catch {\n                case t: Throwable => convertThrowable(s\"Error list columns for ${table.name}\", t)\n                    table.error = ExceptionUtils.getRootCauseMessage(t)\n                    errors.append(convertThrowable(s\"Error list columns for ${table.name}\", t))\n                    return\n            }\n            table.columns = columns\n        }\n\n        tableSet.foreach(table => {\n            processTable(table)\n        })\n\n        val res = ZtoolsSqlInfo(tableSet.toArray, errors.toArray, profilingResult.toArray)\n        val objectMapper = new ObjectMapper().setVisibility(PropertyAccessor.FIELD, Visibility.ANY).writerWithDefaultPrettyPrinter()\n        objectMapper.writeValueAsString(res)\n    }\n\n    def ztoolsPrintResult(): Unit = {\n        val ztoolsSqlResult = calcZtoolsSqlSchemas()\n        println(\"---ztools-sql---\")\n        println(ztoolsSqlResult)\n        println(\"---ztools-sql---\")\n    }\n\n    ztoolsPrintResult()\n}",
   "id": "",
   "dateCreated": "2023-04-25 16:59:39.577",
   "config": {
    "tableHide": true,
    "editorHide": true
   },
   "dateStarted": "2023-04-29 10:51:17.865",
   "dateUpdated": "2023-04-29 10:51:25.112",
   "dateFinished": "2023-04-29 10:51:25.112",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "<console>:371: \u001b[33mwarning: \u001b[0ma pure expression does nothing in statement position; you may be omitting necessary parentheses\n               result\n               ^\n<console>:546: \u001b[33mwarning: \u001b[0mcomparing values of types Long and Null using `!=' will always yield true\n             val startTime = if (initStartTime != null)\n                                               ^\n---ztools-scala---\n{\"variables\":{\"res18\":{\"type\":\"Unit\"},\"res24\":{\"type\":\"Unit\"},\"spark\":{\"type\":\"org.apache.spark.sql.SparkSession\",\"value\":{\"sparkContext\":{\"type\":\"org.apache.spark.SparkContext\",\"value\":{\"appName()\":\"48f0e575-db26-41ac-9910-3937337a502f\",\"sparkTime\":{\"type\":\"Long\",\"value\":1682758687139},\"applicationId()\":{\"type\":\"String\",\"value\":\"local-1682758688989\"},\"applicationAttemptId()\":{\"type\":\"Option[String]\",\"value\":\"None\"},\"sparkUser\":{\"type\":\"String\",\"value\":\"zeppelin\"}},\"time\":11},\"version()\":\"2.4.5\"},\"time\":40},\"csvHotel\":{\"type\":\"String\",\"value\":\"/data/tp/Hotel.csv\",\"time\":2},\"df\":{\"type\":\"org.apache.spark.sql.Dataset\",\"value\":{\"getStorageLevel()\":{\"type\":\"org.apache.spark.storage.StorageLevel\",\"value\":\"StorageLevel(disk, memory, deserialized, 1 replicas)\"},\"schema()\":{\"type\":\"org.apache.spark.sql.types.StructType\",\"length\":16,\"value\":[{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"hotel_ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Reserve ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"pais\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"estado_reserva\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Room ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Tipo de Quarto\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"rate_plan\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_reserva\"},\"dataType\":{\"type\":null,\"value\":\"timestamp\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_chegada\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"data_partida\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"num_noites\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"ocupacao\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"criancas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Bebés\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"preco_euros\"},\"dataType\":{\"type\":null,\"value\":\"double\"}}}]}},\"time\":6},\"res20\":{\"type\":\"Unit\"},\"res14\":{\"type\":\"Unit\"},\"lastException\":{\"type\":\"Throwable\",\"value\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`t.hotel_ID`' given input columns: [qr.data_reserva, qr.criancas, qr.Tipo de Quarto, qr.preco_euros, qr.Bebés, qr.num_noites, qr.data_chegada, t.capacidade_max_adultos, t.Capacidade máxima de camas extra (crianças), qr.data_partida, qr.Room ID, qr.hotel_ID, t.capacidade_max_bebes, t.Hotel ID, t.Capacidade máxima de camas extra, t.Capacidade mínima, qr.estado_reserva, qr.pais, qr.rate_plan, t.Capacidade máxima de berços extra, t.Capacidade mínima de crianças, t.capacidade_maxima, qr.ocupacao, t.Capacidade mínima de adultos, t.Room ID, qr.adultos, t.quantidade, qr.Reserve ID, t.tipo_quarto, t.capacidade_max_criancas]; line 1 pos 432;\\n'Project ['qr.pais, 'qr.estado_reserva, 'qr.rate_plan, 'qr.data_reserva, 'qr.data_partida, 'qr.num_noites, 'qr.ocupacao, 'qr.adultos, 'qr.criancas, 'qr.preco_euros, 'qr.data_chegada, 't.tipo_quarto, 't.quantidade, 't.capacidade_maxima, 't.capacidade_max_adultos, 't.capacidade_max_criancas, 't.capacidade_max_bebes, 'h.localizacao, 'h.estrelas, 'h.idade_max_criancas, 'h.idade_max_bebes, 'h.qtd_quartos]\\n+- 'Join Inner, ('h.hotel_ID = 'qr.hotel_ID)\\n   :- 'Join Inner, (('t.hotel_ID = hotel_ID#4410) && ('t.room_ID = 'qr.room_ID))\\n   :  :- SubqueryAlias `qr`\\n   :  :  +- SubqueryAlias `quartos_reservados`\\n   :  :     +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, adultos#4563, criancas#4580, Bebés#4392, Preço (€)#4393 AS preco_euros#4597]\\n   :  :        +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, adultos#4563, Crianças#4391 AS criancas#4580, Bebés#4392, Preço (€)#4393]\\n   :  :           +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, ocupacao#4546, Adultos#4390 AS adultos#4563, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :              +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, num_noites#4529, Ocupação#4389 AS ocupacao#4546, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                 +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, data_partida#4512, Número de noites#4388 AS num_noites#4529, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                    +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, data_chegada#4495, Data de partida#4387 AS data_partida#4512, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                       +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, data_reserva#4478, Data chegada#4386 AS data_chegada#4495, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                          +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, rate_plan#4461, Data da reserva#4385 AS data_reserva#4478, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                             +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384 AS rate_plan#4461, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                +- Project [hotel_ID#4410, Reserve ID#4379, pais#4427, Estado da reserva#4381 AS estado_reserva#4444, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                   +- Project [hotel_ID#4410, Reserve ID#4379, País#4380 AS pais#4427, Estado da reserva#4381, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                      +- Project [Hotel ID#4378 AS hotel_ID#4410, Reserve ID#4379, País#4380, Estado da reserva#4381, Room ID#4382, Tipo de Quarto#4383, RatePlan#4384, Data da reserva#4385, Data chegada#4386, Data de partida#4387, Número de noites#4388, Ocupação#4389, Adultos#4390, Crianças#4391, Bebés#4392, Preço (€)#4393]\\n   :  :                                         +- Relation[Hotel ID#4378,Reserve ID#4379,País#4380,Estado da reserva#4381,Room ID#4382,Tipo de Quarto#4383,RatePlan#4384,Data da reserva#4385,Data chegada#4386,Data de partida#4387,Número de noites#4388,Ocupação#4389,Adultos#4390,Crianças#4391,Bebés#4392,Preço (€)#4393] csv\\n   :  +- SubqueryAlias `t`\\n   :     +- SubqueryAlias `tipologias`\\n   :        +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, capacidade_max_criancas#2665, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587 AS capacidade_max_bebes#2680, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :           +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585 AS capacidade_max_criancas#2665, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :              +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, capacidade_maxima#2635, Capacidade mínima#2582, Capacidade máxima de adultos#2583 AS capacidade_max_adultos#2650, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                 +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, quantidade#2620, Capacidade máxima#2581 AS capacidade_maxima#2635, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                    +- Project [Hotel ID#2577, Room ID#2578, tipo_quarto#2605, Quantidade#2580 AS quantidade#2620, Capacidade máxima#2581, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                       +- Project [Hotel ID#2577, Room ID#2578, Tipo de quarto#2579 AS tipo_quarto#2605, Quantidade#2580, Capacidade máxima#2581, Capacidade mínima#2582, Capacidade máxima de adultos#2583, Capacidade mínima de adultos#2584, Capacidade máxima de crianças#2585, Capacidade mínima de crianças#2586, Capacidade máxima de bebés#2587, Capacidade máxima de camas extra#2588, Capacidade máxima de camas extra (crianças)#2589, Capacidade máxima de berços extra#2590]\\n   :                          +- Relation[Hotel ID#2577,Room ID#2578,Tipo de quarto#2579,Quantidade#2580,Capacidade máxima#2581,Capacidade mínima#2582,Capacidade máxima de adultos#2583,Capacidade mínima de adultos#2584,Capacidade máxima de crianças#2585,Capacidade mínima de crianças#2586,Capacidade máxima de bebés#2587,Capacidade máxima de camas extra#2588,Capacidade máxima de camas extra (crianças)#2589,Capacidade máxima de berços extra#2590] csv\\n   +- SubqueryAlias `h`\\n      +- SubqueryAlias `hotel`\\n         +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, idade_max_bebes#2359, hora_max_checkin#2367, Quantidade de quartos#2319 AS qtd_quartos#2375]\\n            +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, idade_max_bebes#2359, Hora máxima de check-in#2318 AS hora_max_checkin#2367, Quantidade de quartos#2319]\\n               +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, idade_max_criancas#2351, Idade Máxima de Bebés (Meses)#2317 AS idade_max_bebes#2359, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                  +- Project [hotel_ID#2327, localizacao#2335, estrelas#2343, Idade Máxima de Crianças (Anos)#2316 AS idade_max_criancas#2351, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                     +- Project [hotel_ID#2327, localizacao#2335, Estrelas#2315 AS estrelas#2343, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                        +- Project [hotel_ID#2327, Localização#2314 AS localizacao#2335, Estrelas#2315, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                           +- Project [Hotel ID#2313 AS hotel_ID#2327, Localização#2314, Estrelas#2315, Idade Máxima de Crianças (Anos)#2316, Idade Máxima de Bebés (Meses)#2317, Hora máxima de check-in#2318, Quantidade de quartos#2319]\\n                              +- Relation[Hotel ID#2313,Localização#2314,Estrelas#2315,Idade Máxima de Crianças (Anos)#2316,Idade Máxima de Bebés (Meses)#2317,Hora máxima de check-in#2318,Quantidade de quartos#2319] csv\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:44)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:64)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:66)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:68)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:70)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:72)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:74)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:76)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:78)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:80)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:82)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:84)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:86)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:88)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:90)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:92)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:94)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:96)\\n\\tat $line148955214746.$read$$iw$$iw$$iw$$iw.<init>(<console>:98)\\n\\tat $line148955214746.$read$$iw$$iw$$iw.<init>(<console>:100)\\n\\tat $line148955214746.$read$$iw$$iw.<init>(<console>:102)\\n\\tat $line148955214746.$read$$iw.<init>(<console>:104)\\n\\tat $line148955214746.$read.<init>(<console>:106)\\n\\tat $line148955214746.$read$.<init>(<console>:110)\\n\\tat $line148955214746.$read$.<clinit>(<console>)\\n\\tat $line148955214746.$eval$.$print$lzycompute(<console>:7)\\n\\tat $line148955214746.$eval$.$print(<console>:6)\\n\\tat $line148955214746.$eval.$print(<console>)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\\n\\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\\n\\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\\n\\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\\n\\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\\n\\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\\n\\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\\n\\tat org.apache.zeppelin.spark.SparkScala211Interpreter.scalaInterpret(SparkScala211Interpreter.scala:129)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:120)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:111)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\\n\\tat scala.Console$.withOut(Console.scala:65)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter._interpret$1(BaseSparkScalaInterpreter.scala:111)\\n\\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.interpret(BaseSparkScalaInterpreter.scala:146)\\n\\tat org.apache.zeppelin.spark.SparkInterpreter.internalInterpret(SparkInterpreter.java:183)\\n\\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\\n\\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\\n\\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:852)\\n\\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:744)\\n\\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\\n\\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\\n\\tat org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\",\"time\":5},\"res17\":{\"type\":\"org.apache.spark.sql.Dataset\",\"value\":{\"getStorageLevel()\":{\"type\":\"org.apache.spark.storage.StorageLevel\",\"value\":\"StorageLevel(disk, memory, deserialized, 1 replicas)\"},\"schema()\":{\"type\":\"org.apache.spark.sql.types.StructType\",\"length\":14,\"value\":[{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Hotel ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Room ID\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"tipo_quarto\"},\"dataType\":{\"type\":null,\"value\":\"string\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"quantidade\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_maxima\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima de adultos\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_criancas\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade mínima de crianças\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"capacidade_max_bebes\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de camas extra\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de camas extra (crianças)\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}},{\"type\":\"org.apache.spark.sql.types.StructField\",\"value\":{\"nullable\":{\"type\":null,\"value\":true},\"name\":{\"type\":null,\"value\":\"Capacidade máxima de berços extra\"},\"dataType\":{\"type\":null,\"value\":\"integer\"}}}]}},\"time\":1},\"csvQ_Reservados\":{\"type\":\"String\",\"value\":\"/data/tp/Quartos_Reservados.csv\",\"time\":1},\"colums\":{\"type\":\"Array[String]\",\"length\":7,\"value\":[{\"type\":\"java.lang.String\",\"value\":\"hotel_ID\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"localizacao\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"estrelas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_criancas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_bebes\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"hora_max_checkin\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"qtd_quartos\",\"time\":0}],\"time\":1},\"res25\":{\"type\":\"Unit\"},\"res19\":{\"type\":\"Unit\"},\"sc\":{\"ref\":\"spark.sparkContext\"},\"csvTipologias\":{\"type\":\"String\",\"value\":\"/data/tp/Tipologias.csv\",\"time\":1},\"res22\":{\"type\":\"Unit\"},\"res16\":{\"type\":\"Unit\"},\"csvFacilities\":{\"type\":\"String\",\"value\":\"/data/tp/Facilities.csv\",\"time\":3},\"res21\":{\"type\":\"Unit\"},\"atributos\":{\"type\":\"Array[String]\",\"length\":7,\"value\":[{\"type\":\"java.lang.String\",\"value\":\"hotel_ID\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"localizacao\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"estrelas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_criancas\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"idade_max_bebes\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"hora_max_checkin\",\"time\":0},{\"type\":\"java.lang.String\",\"value\":\"qtd_quartos\",\"time\":0}],\"time\":6}},\"errors\":[]}\n---ztools-scala---\n---ztools-sql---\n{\n  \"tables\" : [ {\n    \"name\" : \"hotel\",\n    \"databaseName\" : \"\",\n    \"columns\" : [ {\n      \"name\" : \"hotel_ID\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"localizacao\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"estrelas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_criancas\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"idade_max_bebes\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    }, {\n      \"name\" : \"hora_max_checkin\",\n      \"columnType\" : \"string\",\n      \"description\" : null\n    }, {\n      \"name\" : \"qtd_quartos\",\n      \"columnType\" : \"int\",\n      \"description\" : null\n    } ],\n    \"error\" : null\n  } ],\n  \"errors\" : [ ],\n  \"profiling\" : [ {\n    \"request\" : \"DESCRIBE TABLE `hotel`\",\n    \"time\" : 35\n  } ],\n  \"appendOutput\" : true\n}\n---ztools-sql---\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {},
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "import org.apache.spark.sql.functions._\n//Leitura de dados Facilities\nval csvFacilities = \"/data/tp/Facilities.csv\"\n\nval df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\") // set this to true if your CSV file has header\n  .option(\"inferSchema\", \"true\") // set this to true if you want Spark to infer the schema\n  .option(\"delimiter\", \";\") // ; is the separator\n  .load(csvFacilities)\n\nval atributos = df.columns\nprintln(\"Atributos:\")\natributos.foreach(println)\n//val summary_facilities = df.describe()\n//summary_facilities.show()\n/*    summary_facilities.coalesce(1).write\n .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"delimiter\", \",\")\n    .mode(\"overwrite\")\n    .save(\"/data/tp/summary_facilities.csv\")\n*/\n//df.printSchema()\n//df.show()\n",
   "id": "",
   "dateCreated": "2023-04-23 23:44:45.859",
   "config": {
    "tableHide": false
   },
   "dateStarted": "2023-04-29 11:01:43.056",
   "dateUpdated": "2023-04-29 11:01:44.108",
   "dateFinished": "2023-04-29 11:01:44.107",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "Atributos:\nHotel ID\nFacility ID\nNome\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mcsvFacilities\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Facilities.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [Hotel ID: int, Facility ID: int ... 1 more field]\n\u001b[1m\u001b[34matributos\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(Hotel ID, Facility ID, Nome)\n"
     }
    ]
   }
  },
  {
   "user": "anonymous",
   "config": {
    "colWidth": 12,
    "fontSize": 9,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {},
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "jobName": "paragraph_1563110258183_1613653816",
   "id": "20190714-161738_1950435706",
   "dateCreated": "2019-07-14T16:17:38+0300",
   "status": "FINISHED",
   "progressUpdateIntervalMs": 500,
   "focus": true,
   "$$hashKey": "object:394",
   "text": "import org.apache.spark.sql.functions._\n//Leitura de dados Hotel\nval csvHotel = \"/data/tp/Hotel.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvHotel)\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"Localização\", \"localizacao\")\n        .withColumnRenamed(\"Estrelas\", \"estrelas\")\n        .withColumnRenamed(\"Idade Máxima de Crianças (Anos)\", \"idade_max_criancas\")\n        .withColumnRenamed(\"Idade Máxima de Bebés (Meses)\", \"idade_max_bebes\")\n        .withColumnRenamed(\"Hora máxima de check-in\", \"hora_max_checkin\")\n        .withColumnRenamed(\"Quantidade de quartos\", \"qtd_quartos\")\n\n//val colums = df.columns\n//println(\"colums:\")\n//colums.foreach(println)\ndf.createOrReplaceTempView(\"Hotel\")\ndf.cache();\n//df.describe().show()\n//df.show()\n\n",
   "dateStarted": "2023-04-29 11:02:01.645",
   "dateUpdated": "2023-04-29 11:02:02.609",
   "dateFinished": "2023-04-29 11:02:02.609",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mcsvHotel\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Hotel.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hotel_ID: int, localizacao: string ... 5 more fields]\n\u001b[1m\u001b[34mres43\u001b[0m: \u001b[1m\u001b[32mdf.type\u001b[0m = [hotel_ID: int, localizacao: string ... 5 more fields]\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados Tipologias\nval csvTipologias = \"/data/tp/Tipologias.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvTipologias)\n        .withColumnRenamed(\"Room ID\", \"room_ID\")\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"Tipo de quarto\", \"tipo_quarto\")\n        .withColumnRenamed(\"Quantidade\", \"quantidade\")\n        .withColumnRenamed(\"Capacidade máxima\", \"capacidade_maxima\")\n        .withColumnRenamed(\"Capacidade máxima de adultos\", \"capacidade_max_adultos\")\n        .withColumnRenamed(\"Capacidade máxima de crianças\", \"capacidade_max_criancas\")\n        .withColumnRenamed(\"Capacidade máxima de bebés\", \"capacidade_max_bebes\")\n\ndf.select(\n    col(\"tipo_quarto\"),\n    col(\"quantidade\"),\n    col(\"capacidade_maxima\"),\n    col(\"capacidade_max_adultos\"),\n    col(\"capacidade_max_criancas\"),\n    col(\"capacidade_max_bebes\")\n  ).describe().show()\ndf.createOrReplaceTempView(\"Tipologias\")\ndf.cache();\n//df.show()\n",
   "id": "",
   "dateCreated": "2023-04-24 21:16:30.526",
   "config": {},
   "dateStarted": "2023-04-29 11:10:43.934",
   "dateUpdated": "2023-04-29 11:10:46.532",
   "dateFinished": "2023-04-29 11:10:46.531",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+--------------------+-----------------+------------------+----------------------+-----------------------+--------------------+\n|summary|         tipo_quarto|       quantidade| capacidade_maxima|capacidade_max_adultos|capacidade_max_criancas|capacidade_max_bebes|\n+-------+--------------------+-----------------+------------------+----------------------+-----------------------+--------------------+\n|  count|                 741|              741|               741|                   741|                    741|                 741|\n|   mean|                null|5.224021592442645|               3.0|     2.748987854251012|     1.0769230769230769|  0.5951417004048583|\n| stddev|                null|8.710572359607925|1.4275285768900876|    1.3453180779776073|     1.3767447822404102|  0.6415168996967767|\n|    min|   Estúdio com Va...|                0|                 1|                     1|                      0|                   0|\n|    max|                Água|               90|                10|                    10|                      8|                   3|\n+-------+--------------------+-----------------+------------------+----------------------+-----------------------+--------------------+\n\n\u001b[1m\u001b[34mcsvTipologias\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Tipologias.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hotel_ID: int, room_ID: int ... 12 more fields]\n\u001b[1m\u001b[34mres51\u001b[0m: \u001b[1m\u001b[32mdf.type\u001b[0m = [hotel_ID: int, room_ID: int ... 12 more fields]\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados Quartos Reservados\nval csvQ_Reservados= \"/data/tp/Quartos_Reservados.csv\"\n\nval df = spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"delimiter\", \";\")\n        .load(csvQ_Reservados)\n        .withColumnRenamed(\"Room ID\", \"room_ID\")\n        .withColumnRenamed(\"Hotel ID\", \"hotel_ID\")\n        .withColumnRenamed(\"País\", \"pais\")\n        .withColumnRenamed(\"Estado da reserva\", \"estado_reserva\")\n        .withColumnRenamed(\"RatePlan\", \"rate_plan\")\n        .withColumnRenamed(\"Data da reserva\", \"data_reserva\")\n        .withColumnRenamed(\"Data chegada\", \"data_chegada\")\n        .withColumnRenamed(\"Data de partida\", \"data_partida\")\n        .withColumnRenamed(\"Número de noites\", \"num_noites\")\n        .withColumnRenamed(\"Ocupação\", \"ocupacao\")\n        .withColumnRenamed(\"Adultos\", \"adultos\")\n        .withColumnRenamed(\"Crianças\", \"criancas\")\n        .withColumnRenamed(\"Preço (€)\", \"preco_euros\")\n\ndf.createOrReplaceTempView(\"Quartos_Reservados\")\ndf.cache()\n\ndf.select(\"pais\", \"estado_reserva\", \"rate_plan\", \"data_reserva\", \"data_chegada\", \"data_partida\", \"num_noites\", \"ocupacao\", \"adultos\", \"criancas\", \"preco_euros\").describe().show()\n//df.show()",
   "id": "",
   "dateCreated": "2023-04-24 21:18:31.653",
   "config": {},
   "dateStarted": "2023-04-29 11:12:14.784",
   "dateUpdated": "2023-04-29 11:12:23.169",
   "dateFinished": "2023-04-29 11:12:23.169",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n|summary|   pais|estado_reserva|           rate_plan|data_chegada|data_partida|       num_noites|          ocupacao|           adultos|           criancas|       preco_euros|\n+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n|  count|  25105|         25105|               25105|       25105|       25105|            25105|             25105|             25105|              25105|             25105|\n|   mean|   null|          null|                null|        null|        null|2.282015534754033|1.8697868950408285| 2.041625174268074| 0.0614618601872137|243.33035643239006|\n| stddev|   null|          null|                null|        null|        null|4.682108190781085|0.6624708635044815|1.0247398139530792|0.29563994588421044| 373.5384599086867|\n|    min|APO/FPO|     Cancelado| PROGRAMA PÁSCOA ...|  01/01/2022|  01/01/2023|                1|                 1|                 1|                  0|               0.0|\n|    max|Áustria|     Registado|    Winter Promotion|  31/12/2022|  31/12/2023|              481|                 9|                30|                  4|           20683.0|\n+-------+-------+--------------+--------------------+------------+------------+-----------------+------------------+------------------+-------------------+------------------+\n\n\u001b[1m\u001b[34mcsvQ_Reservados\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Quartos_Reservados.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hotel_ID: int, Reserve ID: int ... 14 more fields]\n"
     }
    ]
   }
  },
  {
   "settings": {
    "params": {
     "bdtMeta": {
      "ZTOOLS_DATA_FRAMES": [
       {
        "columns": [
         {
          "name": "hotel_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Reserve ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "pais",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "estado_reserva",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "room_ID",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Tipo de Quarto",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "rate_plan",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_reserva",
          "tpe": {
           "presentableName": "timestamp"
          },
          "nullable": true
         },
         {
          "name": "data_chegada",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "data_partida",
          "tpe": {
           "presentableName": "string"
          },
          "nullable": true
         },
         {
          "name": "num_noites",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "ocupacao",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "adultos",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "criancas",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "Bebés",
          "tpe": {
           "presentableName": "integer"
          },
          "nullable": true
         },
         {
          "name": "preco_euros",
          "tpe": {
           "presentableName": "double"
          },
          "nullable": true
         }
        ]
       }
      ]
     }
    },
    "forms": {}
   },
   "apps": [],
   "status": "FINISHED",
   "text": "//Leitura de dados externos Feriados\nval csvFeriados= \"/data/tp/Feriados.csv\"\n\nval df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\") // set this to true if your CSV file has header\n  .option(\"inferSchema\", \"true\") // set this to true if you want Spark to infer the schema\n  .option(\"delimiter\", \";\") // ; is the separator\n  .load(csvFeriados)\n\ndf.show()",
   "id": "",
   "dateCreated": "2023-04-24 22:00:30.321",
   "config": {},
   "dateStarted": "2023-04-29 11:03:12.393",
   "dateUpdated": "2023-04-29 11:03:13.664",
   "dateFinished": "2023-04-29 11:03:13.663",
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----------+---+---------+-----+---------+----+----------+------------------+\n|      Date|day|dayOfWeek|month|trimester|year|is_holiday|portugueseWeekName|\n+----------+---+---------+-----+---------+----+----------+------------------+\n|01/01/2022|  1|        7|    1|        1|2022|         1|            S�bado|\n|02/01/2022|  2|        1|    1|        1|2022|         0|           Domingo|\n|03/01/2022|  3|        2|    1|        1|2022|         0|     Segunda-feira|\n|04/01/2022|  4|        3|    1|        1|2022|         0|       Ter�a-feira|\n|05/01/2022|  5|        4|    1|        1|2022|         0|      Quarta-feira|\n|06/01/2022|  6|        5|    1|        1|2022|         0|      Quinta-feira|\n|07/01/2022|  7|        6|    1|        1|2022|         0|       Sexta-feira|\n|08/01/2022|  8|        7|    1|        1|2022|         0|            S�bado|\n|09/01/2022|  9|        1|    1|        1|2022|         0|           Domingo|\n|10/01/2022| 10|        2|    1|        1|2022|         0|     Segunda-feira|\n|11/01/2022| 11|        3|    1|        1|2022|         0|       Ter�a-feira|\n|12/01/2022| 12|        4|    1|        1|2022|         0|      Quarta-feira|\n|13/01/2022| 13|        5|    1|        1|2022|         0|      Quinta-feira|\n|14/01/2022| 14|        6|    1|        1|2022|         0|       Sexta-feira|\n|15/01/2022| 15|        7|    1|        1|2022|         0|            S�bado|\n|16/01/2022| 16|        1|    1|        1|2022|         0|           Domingo|\n|17/01/2022| 17|        2|    1|        1|2022|         0|     Segunda-feira|\n|18/01/2022| 18|        3|    1|        1|2022|         0|       Ter�a-feira|\n|19/01/2022| 19|        4|    1|        1|2022|         0|      Quarta-feira|\n|20/01/2022| 20|        5|    1|        1|2022|         0|      Quinta-feira|\n+----------+---+---------+-----+---------+----+----------+------------------+\nonly showing top 20 rows\n\n\u001b[1m\u001b[34mcsvFeriados\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/tp/Feriados.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [Date: string, day: int ... 6 more fields]\n"
     }
    ]
   }
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}